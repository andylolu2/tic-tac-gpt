@inproceedings{bender2020climbing,
  author    = {Bender, Emily M and Koller, Alexander},
  booktitle = {Proceedings of the 58th annual meeting of the association for computational linguistics},
  pages     = {5185--5198},
  title     = {Climbing towards NLU: On meaning, form, and understanding in the age of data},
  year      = {2020}
}
@article{elhage2021mathematical,
  author  = {Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
  journal = {Transformer Circuits Thread},
  note    = {https://transformer-circuits.pub/2021/framework/index.html},
  title   = {A Mathematical Framework for Transformer Circuits},
  year    = {2021}
}
@article{elhage2022solu,
  author  = {Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Nanda, Neel and Henighan, Tom and Johnston, Scott and ElShowk, Sheer and Joseph, Nicholas and DasSarma, Nova and Mann, Ben and Hernandez, Danny and Askell, Amanda and Ndousse, Kamal and Jones, Andy and Drain, Dawn and Chen, Anna and Bai, Yuntao and Ganguli, Deep and Lovitt, Liane and Hatfield-Dodds, Zac and Kernion, Jackson and Conerly, Tom and Kravec, Shauna and Fort, Stanislav and Kadavath, Saurav and Jacobson, Josh and Tran-Johnson, Eli and Kaplan, Jared and Clark, Jack and Brown, Tom and McCandlish, Sam and Amodei, Dario and Olah, Christopher},
  journal = {Transformer Circuits Thread},
  note    = {https://transformer-circuits.pub/2022/solu/index.html},
  title   = {Softmax Linear Units},
  year    = {2022}
}
@article{linear-orthello-gpt,
  author  = {Nanda, Neel and Lee, Andrew and Wattenberg, Martin},
  journal = {arXiv preprint arXiv:2309.00941},
  title   = {Emergent linear representations in world models of self-supervised sequence models},
  year    = {2023}
}
@article{loshchilov2017decoupled,
  author  = {Loshchilov, Ilya and Hutter, Frank},
  journal = {arXiv preprint arXiv:1711.05101},
  title   = {Decoupled weight decay regularization},
  year    = {2017}
}
@article{merrill2021provable,
  author    = {Merrill, William and Goldberg, Yoav and Schwartz, Roy and Smith, Noah A},
  journal   = {Transactions of the Association for Computational Linguistics},
  pages     = {1047--1060},
  publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦},
  title     = {Provable limitations of acquiring meaning from ungrounded form: What will future language models understand?},
  volume    = {9},
  year      = {2021}
}
@article{olah2020zoom,
  author  = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  doi     = {10.23915/distill.00024.001},
  journal = {Distill},
  note    = {https://distill.pub/2020/circuits/zoom-in},
  title   = {Zoom In: An Introduction to Circuits},
  year    = {2020}
}
@article{orthello-gpt,
  author  = {Li, Kenneth and Hopkins, Aspen K and Bau, David and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  journal = {arXiv preprint arXiv:2210.13382},
  title   = {Emergent world representations: Exploring a sequence model trained on a synthetic task},
  year    = {2022}
}
@article{radford2019language,
  author  = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal = {OpenAI blog},
  number  = {8},
  pages   = {9},
  title   = {Language models are unsupervised multitask learners},
  volume  = {1},
  year    = {2019}
}
@article{toshniwal2021learning,
  author  = {Toshniwal, Shubham and Wiseman, Sam and Livescu, Karen and Gimpel, Kevin},
  journal = {arXiv preprint arXiv:2102.13249},
  title   = {Learning chess blindfolded: Evaluating language models on state tracking},
  volume  = {2},
  year    = {2021}
}
@article{vaswani2017attention,
  author  = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal = {Advances in neural information processing systems},
  title   = {Attention is all you need},
  volume  = {30},
  year    = {2017}
}
@inproceedings{xiong2020layer,
  author       = {Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tieyan},
  booktitle    = {International Conference on Machine Learning},
  organization = {PMLR},
  pages        = {10524--10533},
  title        = {On layer normalization in the transformer architecture},
  year         = {2020}
}
