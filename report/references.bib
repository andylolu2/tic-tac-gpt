@article{bau2018identifying,
  author  = {Bau, Anthony and Belinkov, Yonatan and Sajjad, Hassan and Durrani, Nadir and Dalvi, Fahim and Glass, James},
  journal = {arXiv preprint arXiv:1811.01157},
  title   = {Identifying and controlling important neurons in neural machine translation},
  year    = {2018}
}
@article{bau2020understanding,
  author    = {Bau, David and Zhu, Jun-Yan and Strobelt, Hendrik and Lapedriza, Agata and Zhou, Bolei and Torralba, Antonio},
  journal   = {Proceedings of the National Academy of Sciences},
  number    = {48},
  pages     = {30071--30078},
  publisher = {National Acad Sciences},
  title     = {Understanding the role of individual units in a deep neural network},
  volume    = {117},
  year      = {2020}
}
@inproceedings{bender2020climbing,
  author    = {Bender, Emily M and Koller, Alexander},
  booktitle = {Proceedings of the 58th annual meeting of the association for computational linguistics},
  pages     = {5185--5198},
  title     = {Climbing towards NLU: On meaning, form, and understanding in the age of data},
  year      = {2020}
}
@article{cammarata2020thread:,
  author  = {Cammarata, Nick and Carter, Shan and Goh, Gabriel and Olah, Chris and Petrov, Michael and Schubert, Ludwig and Voss, Chelsea and Egan, Ben and Lim, Swee Kiat},
  doi     = {10.23915/distill.00024},
  journal = {Distill},
  note    = {https://distill.pub/2020/circuits},
  title   = {Thread: Circuits},
  year    = {2020}
}
@article{elhage2021mathematical,
  author  = {Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
  journal = {Transformer Circuits Thread},
  note    = {https://transformer-circuits.pub/2021/framework/index.html},
  title   = {A Mathematical Framework for Transformer Circuits},
  year    = {2021}
}
@article{elhage2022solu,
  author  = {Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Nanda, Neel and Henighan, Tom and Johnston, Scott and ElShowk, Sheer and Joseph, Nicholas and DasSarma, Nova and Mann, Ben and Hernandez, Danny and Askell, Amanda and Ndousse, Kamal and Jones, Andy and Drain, Dawn and Chen, Anna and Bai, Yuntao and Ganguli, Deep and Lovitt, Liane and Hatfield-Dodds, Zac and Kernion, Jackson and Conerly, Tom and Kravec, Shauna and Fort, Stanislav and Kadavath, Saurav and Jacobson, Josh and Tran-Johnson, Eli and Kaplan, Jared and Clark, Jack and Brown, Tom and McCandlish, Sam and Amodei, Dario and Olah, Christopher},
  journal = {Transformer Circuits Thread},
  note    = {https://transformer-circuits.pub/2022/solu/index.html},
  title   = {Softmax Linear Units},
  year    = {2022}
}
@article{elhage2022toy,
  author  = {Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and others},
  journal = {arXiv preprint arXiv:2209.10652},
  title   = {Toy models of superposition},
  year    = {2022}
}
@article{gurnee2023finding,
  author  = {Gurnee, Wes and Nanda, Neel and Pauly, Matthew and Harvey, Katherine and Troitskii, Dmitrii and Bertsimas, Dimitris},
  journal = {arXiv preprint arXiv:2305.01610},
  title   = {Finding neurons in a haystack: Case studies with sparse probing},
  year    = {2023}
}
@article{heimersheim2023circuit,
  author  = {Heimersheim, Stefan and Janiak, Jett},
  journal = {URL: https://www. alignmentforum. org/posts/u6KXXmKFbXfWzoAXn/acircuit-for-python-docstrings-in-a-4-layer-attention-only},
  title   = {A circuit for Python docstrings in a 4-layer attention-only transformer},
  year    = {2023}
}
@article{hu2016network,
  author  = {Hu, Hengyuan and Peng, Rui and Tai, Yu-Wing and Tang, Chi-Keung},
  journal = {arXiv preprint arXiv:1607.03250},
  title   = {Network trimming: A data-driven neuron pruning approach towards efficient deep architectures},
  year    = {2016}
}
@article{karpathy2015visualizing,
  author  = {Karpathy, Andrej and Johnson, Justin and Fei-Fei, Li},
  journal = {arXiv preprint arXiv:1506.02078},
  title   = {Visualizing and understanding recurrent networks},
  year    = {2015}
}
@article{linear-orthello-gpt,
  author  = {Nanda, Neel and Lee, Andrew and Wattenberg, Martin},
  journal = {arXiv preprint arXiv:2309.00941},
  title   = {Emergent linear representations in world models of self-supervised sequence models},
  year    = {2023}
}
@article{loshchilov2017decoupled,
  author  = {Loshchilov, Ilya and Hutter, Frank},
  journal = {arXiv preprint arXiv:1711.05101},
  title   = {Decoupled weight decay regularization},
  year    = {2017}
}
@article{mccoy2023embers,
  author  = {McCoy, R Thomas and Yao, Shunyu and Friedman, Dan and Hardy, Matthew and Griffiths, Thomas L},
  journal = {arXiv preprint arXiv:2309.13638},
  title   = {Embers of autoregression: Understanding large language models through the problem they are trained to solve},
  year    = {2023}
}
@article{meng2022locating,
  author  = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  journal = {Advances in Neural Information Processing Systems},
  pages   = {17359--17372},
  title   = {Locating and editing factual associations in GPT},
  volume  = {35},
  year    = {2022}
}
@article{merrill2021provable,
  author    = {Merrill, William and Goldberg, Yoav and Schwartz, Roy and Smith, Noah A},
  journal   = {Transactions of the Association for Computational Linguistics},
  pages     = {1047--1060},
  publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦},
  title     = {Provable limitations of acquiring meaning from ungrounded form: What will future language models understand?},
  volume    = {9},
  year      = {2021}
}
@article{nanda2023attribution,
  author  = {Nanda, Neel},
  journal = {URL: https://www. neelnanda. io/mechanistic-interpretability/attribution-patching},
  title   = {Attribution patching: Activation patching at industrial scale},
  year    = {2023}
}
@article{nanda2023progress,
  author  = {Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob},
  journal = {arXiv preprint arXiv:2301.05217},
  title   = {Progress measures for grokking via mechanistic interpretability},
  year    = {2023}
}
@article{olah2020zoom,
  author  = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  doi     = {10.23915/distill.00024.001},
  journal = {Distill},
  note    = {https://distill.pub/2020/circuits/zoom-in},
  title   = {Zoom In: An Introduction to Circuits},
  year    = {2020}
}
@article{olsson2022context,
  author  = {Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
  journal = {arXiv preprint arXiv:2209.11895},
  title   = {In-context learning and induction heads},
  year    = {2022}
}
@article{orthello-gpt,
  author  = {Li, Kenneth and Hopkins, Aspen K and Bau, David and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  journal = {arXiv preprint arXiv:2210.13382},
  title   = {Emergent world representations: Exploring a sequence model trained on a synthetic task},
  year    = {2022}
}
@article{radford2019language,
  author  = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal = {OpenAI blog},
  number  = {8},
  pages   = {9},
  title   = {Language models are unsupervised multitask learners},
  volume  = {1},
  year    = {2019}
}
@article{sobol1993sensitivity,
  author  = {Sobo{\'l}, IM},
  journal = {Math. Model. Comput. Exp.},
  pages   = {407},
  title   = {Sensitivity estimates for nonlinear mathematical models},
  volume  = {1},
  year    = {1993}
}
@article{sobol2001global,
  author    = {Sobol, Ilya M},
  journal   = {Mathematics and computers in simulation},
  number    = {1-3},
  pages     = {271--280},
  publisher = {Elsevier},
  title     = {Global sensitivity indices for nonlinear mathematical models and their Monte Carlo estimates},
  volume    = {55},
  year      = {2001}
}
@article{tigges2023linear,
  author  = {Tigges, Curt and Hollinsworth, Oskar John and Geiger, Atticus and Nanda, Neel},
  journal = {arXiv preprint arXiv:2310.15154},
  title   = {Linear representations of sentiment in large language models},
  year    = {2023}
}
@article{toshniwal2021learning,
  author  = {Toshniwal, Shubham and Wiseman, Sam and Livescu, Karen and Gimpel, Kevin},
  journal = {arXiv preprint arXiv:2102.13249},
  title   = {Learning chess blindfolded: Evaluating language models on state tracking},
  volume  = {2},
  year    = {2021}
}
@article{vaswani2017attention,
  author  = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal = {Advances in neural information processing systems},
  title   = {Attention is all you need},
  volume  = {30},
  year    = {2017}
}
@article{vig2020investigating,
  author  = {Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Singer, Yaron and Shieber, Stuart},
  journal = {Advances in neural information processing systems},
  pages   = {12388--12401},
  title   = {Investigating gender bias in language models using causal mediation analysis},
  volume  = {33},
  year    = {2020}
}
@inproceedings{xiong2020layer,
  author       = {Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tieyan},
  booktitle    = {International Conference on Machine Learning},
  organization = {PMLR},
  pages        = {10524--10533},
  title        = {On layer normalization in the transformer architecture},
  year         = {2020}
}
@article{zhou2014object,
  author  = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  journal = {arXiv preprint arXiv:1412.6856},
  title   = {Object detectors emerge in deep scene cnns},
  year    = {2014}
}
