{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from lightning import fabric\n",
    "from transformer_lens import (\n",
    "    HookedTransformer,\n",
    "    HookedTransformerConfig,\n",
    "    FactoredMatrix,\n",
    "    ActivationCache,\n",
    ")\n",
    "from transformer_lens import utils\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier, MultiOutputRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import log_loss\n",
    "from einops import einsum, rearrange, unpack, repeat\n",
    "from matplotlib import cm, colors\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tic_tac_gpt.data import TicTacToeDataset, TicTacToeState, tensor_to_state\n",
    "\n",
    "import tic_tac_gpt.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "torch.set_default_device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = Path(\"out/model/exp23\")\n",
    "\n",
    "with open(checkpoint_dir / \"config.pkl\", \"rb\") as f:\n",
    "    config: HookedTransformerConfig = pickle.load(f)\n",
    "model = HookedTransformer(config)\n",
    "F = fabric.Fabric(precision=\"16-mixed\")\n",
    "state_dict = F.load(checkpoint_dir / \"model_40000.pt\")\n",
    "model.load_and_process_state_dict(state_dict)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_train = TicTacToeDataset.from_file(Path(\"out/dataset/50_50/train.jsonl\"))\n",
    "# ds_test = TicTacToeDataset.from_file(Path(\"out/dataset/50_50/test.jsonl\"))\n",
    "ds_train = TicTacToeDataset.from_file(Path(\"out/dataset/50_50_even/train.jsonl\"))\n",
    "ds_test = TicTacToeDataset.from_file(Path(\"out/dataset/50_50_even/test.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_logit(logit, ax1=None, ax2=None, vmin=None, vmax=None, center: float = 0):\n",
    "    probs = logit\n",
    "    board = [[0] * 3 for _ in range(3)]\n",
    "    special = {}\n",
    "    for i, p in enumerate(probs.tolist()):\n",
    "        if i < 9:\n",
    "            board[i // 3][i % 3] = p\n",
    "        elif i == 9:\n",
    "            special[\"[X]\"] = p\n",
    "        elif i == 10:\n",
    "            special[\"[O]\"] = p\n",
    "        elif i == 11:\n",
    "            special[\"[D]\"] = p\n",
    "\n",
    "    if ax1 is None or ax2 is None:\n",
    "        fig, (ax1, ax2) = plt.subplots(\n",
    "            ncols=2, figsize=(3, 2), gridspec_kw={\"width_ratios\": [1, 0.4]}\n",
    "        )\n",
    "    if vmin is None:\n",
    "        vmin = probs.min().item()\n",
    "    if vmax is None:\n",
    "        vmax = probs.max().item()\n",
    "\n",
    "    sns.heatmap(\n",
    "        board,\n",
    "        ax=ax1,\n",
    "        cmap=\"RdBu\",\n",
    "        center=center,\n",
    "        square=True,\n",
    "        cbar=False,\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "    )\n",
    "    ax1.set(yticks=[], xticks=[])\n",
    "    sns.heatmap(\n",
    "        [[v] for v in special.values()],\n",
    "        ax=ax2,\n",
    "        cmap=\"RdBu\",\n",
    "        center=center,\n",
    "        square=True,\n",
    "        cbar=False,\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "    )\n",
    "    ax2.set(yticks=[], xticks=[])\n",
    "\n",
    "\n",
    "def visualise_board(state):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(state.board, cmap=\"gray\", aspect=\"equal\", vmin=0, vmax=2)\n",
    "    ax.set(xticks=[], yticks=[], title=\"Board\")\n",
    "    return fig\n",
    "\n",
    "\n",
    "(x,) = random.choice(ds_train)\n",
    "game = tensor_to_state(x)\n",
    "print(game)\n",
    "visualise_logit(model(x)[0, len(game)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_games = ds_train[torch.randperm(len(ds_train))[:5000]][0]\n",
    "focus_games_test = ds_test[torch.randperm(len(ds_test))[:5000]][0]\n",
    "focus_games.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, cache = model.run_with_cache(focus_games[:, :-1])\n",
    "cache.compute_head_results()\n",
    "logits_test, cache_test = model.run_with_cache(focus_games_test[:, :-1])\n",
    "cache_test.compute_head_results()\n",
    "print(cache.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kurtosis\n",
    "\n",
    "a = cache[\"resid_mid\", 0]\n",
    "\n",
    "sns.histplot(a.flatten().cpu().numpy(), bins=100)\n",
    "# kurtosis(a.flatten().cpu().numpy(), fisher=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_states(game: torch.Tensor):\n",
    "    for i in range(1, game.shape[0] + 1):\n",
    "        state = tensor_to_state(game[:i])\n",
    "        yield state\n",
    "        if state.result != \"in_progress\":\n",
    "            break\n",
    "\n",
    "\n",
    "def board_to_tensor(board):\n",
    "    return torch.tensor(board).flatten(-2, -1)\n",
    "\n",
    "\n",
    "board_states = extract_states(focus_games[7])\n",
    "for s in board_states:\n",
    "    print(s)\n",
    "    print(board_to_tensor(s.board))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_layer = 0\n",
    "\n",
    "\n",
    "def invert(x):\n",
    "    y = x.clone()\n",
    "    y[x == 1] = 2\n",
    "    y[x == 2] = 1\n",
    "    return y\n",
    "\n",
    "\n",
    "def remap(x):\n",
    "    y = x.clone()\n",
    "    y[x == 0] = 0\n",
    "    y[x == 1] = 1\n",
    "    y[x == 2] = 2\n",
    "    return y\n",
    "\n",
    "\n",
    "def extract_XY(games, cache):\n",
    "    activations = cache[\"resid_mid\", probe_layer]\n",
    "    # activations = cache[\"attn_out\", probe_layer]\n",
    "    # activations = cache[\"z\", probe_layer, \"attn\"][:, :, 2, :]\n",
    "    X, Y = [], []\n",
    "    for game, game_activation in zip(games, activations):\n",
    "        for i, (state, activation) in enumerate(\n",
    "            zip(extract_states(game), game_activation)\n",
    "        ):\n",
    "            X.append(activation)\n",
    "            target = board_to_tensor(state.board)\n",
    "            if i % 2 == 0:\n",
    "                target = invert(target)\n",
    "            # target = remap(target)\n",
    "            Y.append(target)\n",
    "            # Y.append(torch.nn.functional.one_hot(target, 3))\n",
    "    X, Y = torch.stack(X), torch.stack(Y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "X, Y = extract_XY(focus_games, cache)\n",
    "X_test, Y_test = extract_XY(focus_games_test, cache_test)\n",
    "X.shape, Y.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np = X.cpu().numpy()\n",
    "Y_np = Y.cpu().numpy()\n",
    "# Y_np = Y.flatten(-2, -1).cpu().numpy()\n",
    "X_test_np = X_test.cpu().numpy()\n",
    "Y_test_np = Y_test.cpu().numpy()\n",
    "# Y_test_np = Y_test.flatten(-2, -1).cpu().numpy()\n",
    "\n",
    "regressor = MultiOutputRegressor(LinearRegression())\n",
    "regressor.fit(X_np, Y_np)\n",
    "print(regressor.score(X_np, Y_np), regressor.score(X_test_np, Y_test_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np = X.cpu().numpy()\n",
    "Y_np = Y.cpu().numpy()\n",
    "X_test_np = X_test.cpu().numpy()\n",
    "Y_test_np = Y_test.cpu().numpy()\n",
    "\n",
    "classifier = MultiOutputClassifier(LogisticRegression(max_iter=1000))\n",
    "classifier.fit(X_np, Y_np)\n",
    "\n",
    "Y_pred = np.array(classifier.predict_proba(X_np)).transpose(1, 0, 2)\n",
    "Y_test_pred = np.array(classifier.predict_proba(X_test_np)).transpose(1, 0, 2)\n",
    "print(classifier.score(X_np, Y_np), classifier.score(X_test_np, Y_test_np))\n",
    "print(\n",
    "    log_loss(Y_np.reshape(-1), Y_pred.reshape(-1, Y_pred.shape[-1])),\n",
    "    log_loss(Y_test_np.reshape(-1), Y_test_pred.reshape(-1, Y_test_pred.shape[-1])),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attn(x, pattern):\n",
    "    \"\"\"Input shape (nh s s)\"\"\"\n",
    "    fig, axs = plt.subplots(\n",
    "        ncols=pattern.shape[0] + 1,\n",
    "        figsize=(pattern.shape[0] * 4, 4),\n",
    "        gridspec_kw={\"width_ratios\": [1] * pattern.shape[0] + [0.07]},\n",
    "    )\n",
    "    for i in range(pattern.shape[0]):\n",
    "        ax = axs[i]\n",
    "        p = pattern[i]\n",
    "        sns.heatmap(\n",
    "            p,\n",
    "            ax=ax,\n",
    "            cmap=\"viridis\",\n",
    "            square=True,\n",
    "            xticklabels=[f\"\\\\texttt{{{i}}}\" for i in x],\n",
    "            yticklabels=[f\"\\\\texttt{{{i}}}\" for i in x],\n",
    "            cbar=False,\n",
    "            linewidth=0.5,\n",
    "            linecolor=\"black\",\n",
    "            clip_on=False,\n",
    "        )\n",
    "        ax.set(title=f\"Head {i}\")\n",
    "\n",
    "    im = cm.ScalarMappable(cmap=\"viridis\", norm=colors.Normalize(vmin=0, vmax=1))\n",
    "    fig.colorbar(im, cax=axs[-1])\n",
    "    return fig\n",
    "\n",
    "\n",
    "idx = random.randint(0, len(focus_games))\n",
    "game = focus_games[idx]\n",
    "# game = torch.tensor([ds_train.bos_token, 1, 5, 4, 8, 7])\n",
    "print(tensor_to_state(game))\n",
    "\n",
    "_, tmp_cache = model.run_with_cache(game)\n",
    "fig = plot_attn(\n",
    "    TicTacToeDataset.decode(game),\n",
    "    tmp_cache[\"pattern\", 0][0].cpu().numpy(),\n",
    ")\n",
    "# fig.savefig(\"out/figs/attn_pattern.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(x: torch.Tensor, y: torch.Tensor):\n",
    "    x = torch.nn.functional.normalize(x, dim=-1)\n",
    "    y = torch.nn.functional.normalize(y, dim=-1)\n",
    "    return torch.tensordot(x, y, dims=([-1], [-1]))\n",
    "\n",
    "\n",
    "def normalize(x, dim=-1):\n",
    "    x = x - x.mean(dim, keepdim=True)\n",
    "    scale = (x.pow(2).mean(dim, keepdim=True)).sqrt()\n",
    "    return x / scale\n",
    "\n",
    "\n",
    "x = torch.randn(4, 5)\n",
    "y = torch.randn(3, 5)\n",
    "assert (cosine_sim(x, y) == cosine_sim(y, x).T).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_idx = 1\n",
    "\n",
    "seq_activations = [[] for _ in range(ds_train.max_seq_len - 1)]\n",
    "for activations, game in zip(cache[\"resid_mid\", 0], focus_games_test):\n",
    "    # activations = activations[:, head_idx]\n",
    "    for i, (activation, move) in enumerate(zip(activations, game)):\n",
    "        move = move.item()\n",
    "        if move in (0, 1, 2, 3, 4, 5, 6, 7, 8, ds_train.bos_token):\n",
    "            seq_activations[i].append(activation)\n",
    "seq_activations = [torch.stack(s) for s in seq_activations]\n",
    "print([s.shape for s in seq_activations])\n",
    "\n",
    "data = []\n",
    "for i, activations in enumerate(seq_activations):\n",
    "    singulars = torch.linalg.svdvals(activations).cpu().numpy()\n",
    "    singulars = singulars / singulars[0]\n",
    "    for j, s in enumerate(sorted(singulars, reverse=True)):\n",
    "        data.append({\"idx\": j, \"singular\": s, \"position\": str(i)})\n",
    "\n",
    "ax = sns.lineplot(data=pd.DataFrame(data), x=\"idx\", y=\"singular\", hue=\"position\")\n",
    "# for x in (1, 9, 10, 11, 12, 13, 15, 17, 19, 20):\n",
    "for x in (0, 8, 10, 16, 20, 22, 24, 26, 29, 32, 35, 38):\n",
    "    ax.axvline(x, color=\"black\", linestyle=\"--\", alpha=0.5)\n",
    "ax.set(yscale=\"log\", ylim=(1e-3, 1), xlim=(-1, 50))\n",
    "# ax.set(ylim=(None, None), xlim=(-1, 40))\n",
    "# ax.set(yscale=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_1 = torch.tensor([[ds_train.bos_token, 0, 5, 1, 8, 2]])\n",
    "game_2 = torch.tensor([[ds_train.bos_token, 0, 5, 1, 8, 2]])\n",
    "print(tensor_to_state(game_1[0]))\n",
    "print(tensor_to_state(game_2[0]))\n",
    "\n",
    "clean_logits, clean_cache = model.run_with_cache(game_1)\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(game_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_head_vector(corrupted_head_vector, hook, head_index):\n",
    "    corrupted_head_vector[:, -2, head_index, :] = corrupted_cache[hook.name][\n",
    "        :, -1, head_index, :\n",
    "    ]\n",
    "    return corrupted_head_vector\n",
    "\n",
    "\n",
    "def logit_diff(patched_logits):\n",
    "    # terminate_idx = [9, 10, 11]\n",
    "    # other_idx = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "    terminate_idx = [10]\n",
    "    other_idx = [9]\n",
    "    seq_idx = -2\n",
    "    diff_patched = patched_logits[:, seq_idx, terminate_idx].mean(-1) - patched_logits[\n",
    "        :, seq_idx, other_idx\n",
    "    ].mean(-1)\n",
    "    diff_clean = clean_logits[:, seq_idx, terminate_idx].mean(-1) - clean_logits[\n",
    "        :, seq_idx, other_idx\n",
    "    ].mean(-1)\n",
    "    rel_change = diff_patched  # - diff_clean\n",
    "    return rel_change.mean()\n",
    "\n",
    "\n",
    "patched_residual_stream_diff = torch.zeros(\n",
    "    model.cfg.n_layers, model.cfg.n_heads, game_1.shape[1]\n",
    ")\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    for head_index in range(model.cfg.n_heads):\n",
    "        for seq_idx in range(game_1.shape[1]):\n",
    "            hook_fn = partial(patch_head_vector, head_index=head_index)\n",
    "            patched_logits = model.run_with_hooks(\n",
    "                game_1,\n",
    "                fwd_hooks=[(utils.get_act_name(\"v\", layer, \"attn\"), hook_fn)],\n",
    "                return_type=\"logits\",\n",
    "            )\n",
    "            patched_logit_diff = logit_diff(patched_logits)\n",
    "            patched_residual_stream_diff[layer, head_index, seq_idx] = (\n",
    "                patched_logit_diff\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=model.cfg.n_heads, figsize=(8, 8))\n",
    "_min = patched_residual_stream_diff.min().item()\n",
    "_max = patched_residual_stream_diff.max().item()\n",
    "for i, ax in enumerate(axs):\n",
    "    sns.heatmap(\n",
    "        patched_residual_stream_diff[:, i].cpu(),\n",
    "        ax=ax,\n",
    "        cmap=\"RdBu\",\n",
    "        square=True,\n",
    "        center=0,\n",
    "        vmin=_min,\n",
    "        vmax=_max,\n",
    "    )\n",
    "    ax.set(\n",
    "        xlabel=\"Seq\",\n",
    "        ylabel=\"Layer\",\n",
    "        xticklabels=ds_train.decode(game_1[0]),\n",
    "    )\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_idx = random.randint(0, 2048)\n",
    "game_tensor = focus_games[game_idx]\n",
    "# game_tensor = torch.tensor([ds_train.bos_token, 8, 6, 0, 4, 1, 5, 2])\n",
    "# game_tensor = torch.tensor([ds_train.bos_token, 3, 6, 1, 5, 4, 8, 7])\n",
    "game = tensor_to_state(game_tensor)\n",
    "\n",
    "print(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_idx = 1\n",
    "s = len(game) + 1\n",
    "\n",
    "\n",
    "def logit_contribution(activation):\n",
    "    return torch.matmul(activation, model.W_U[:, out_idx])\n",
    "\n",
    "\n",
    "head_results = cache.stack_head_results()\n",
    "head_results = cache.apply_ln_to_stack(head_results)\n",
    "head_results = rearrange(\n",
    "    head_results, \"(l nh) b s d -> b s l nh d\", l=model.cfg.n_layers\n",
    ")\n",
    "\n",
    "out = torch.zeros(model.cfg.n_layers, model.cfg.n_heads, s)\n",
    "for layer_idx in range(model.cfg.n_layers):\n",
    "    for head_idx in range(model.cfg.n_heads):\n",
    "        for seq_idx in range(s):\n",
    "            out[layer_idx, head_idx, seq_idx] = logit_contribution(\n",
    "                head_results[game_idx, seq_idx, layer_idx, head_idx]\n",
    "            )\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    ncols=3,\n",
    "    nrows=s,\n",
    "    figsize=(10, 18),\n",
    "    gridspec_kw={\"width_ratios\": [3, 1, 3]},\n",
    ")\n",
    "# v_min = out.min().item()\n",
    "# v_max = out.max().item()\n",
    "for seq_idx in range(s):\n",
    "    visualise_logit(logits[game_idx, seq_idx], axs[seq_idx, 0], axs[seq_idx, 1])\n",
    "    sns.heatmap(\n",
    "        out[:, :, seq_idx].cpu(),\n",
    "        ax=axs[seq_idx, 2],\n",
    "        cmap=\"RdBu\",\n",
    "        center=0,\n",
    "        square=True,\n",
    "    )\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_idx = len(game) - 2\n",
    "seq_idx = len(game)\n",
    "\n",
    "\n",
    "def logit_contribution(activation, out_idx):\n",
    "    return torch.matmul(activation, model.W_U[:, out_idx])\n",
    "\n",
    "\n",
    "tmp_logits, tmp_cache = model.run_with_cache(game_tensor)\n",
    "all_acts = tmp_cache.get_full_resid_decomposition(pos_slice=seq_idx, apply_ln=True)\n",
    "# all_acts = tmp_cache.apply_ln_to_stack(all_acts, pos_slice=seq_idx)\n",
    "all_acts = all_acts[:, 0, :]\n",
    "\n",
    "heads, neurons, emb, pos, bias = unpack(\n",
    "    all_acts,\n",
    "    [\n",
    "        [model.cfg.n_layers, model.cfg.n_heads],\n",
    "        [model.cfg.n_layers, model.cfg.d_mlp],\n",
    "        [1],\n",
    "        [1],\n",
    "        [1],\n",
    "    ],\n",
    "    \"* d\",\n",
    ")\n",
    "print(game)\n",
    "for name, act in {\"emb\": emb, \"pos\": pos, \"bias\": bias}.items():\n",
    "    logits_contributions = [\n",
    "        logit_contribution(act, i).item() for i in range(model.cfg.d_vocab_out)\n",
    "    ]\n",
    "    print(f\"{name:>4}: {' '.join(f'{x:>5.2f}' for x in logits_contributions)}\")\n",
    "\n",
    "with plt.style.context(\"default\"):\n",
    "    fig, axs = plt.subplots(\n",
    "        ncols=2,\n",
    "        nrows=model.cfg.d_vocab_out,\n",
    "        figsize=(12, 1 * model.cfg.d_vocab_out),\n",
    "        gridspec_kw={\"width_ratios\": [1, 10]},\n",
    "        sharey=\"row\",\n",
    "        sharex=\"col\",\n",
    "    )\n",
    "    for i in range(model.cfg.d_vocab_out):\n",
    "        heads_logits = logit_contribution(heads, i).flatten()\n",
    "        sns.scatterplot(\n",
    "            x=np.arange(heads_logits.numel()),\n",
    "            y=heads_logits.cpu().numpy(),\n",
    "            ax=axs[i, 0],\n",
    "        )\n",
    "        neurons_logits = logit_contribution(neurons, i).flatten()\n",
    "        sns.scatterplot(\n",
    "            x=np.arange(neurons_logits.numel()),\n",
    "            y=neurons_logits.cpu().numpy(),\n",
    "            ax=axs[i, 1],\n",
    "        )\n",
    "        # label outliers\n",
    "        for x, y in enumerate(neurons_logits):\n",
    "            if y.abs() > 0.5:\n",
    "                axs[i, 1].text(x, y, f\"{x}\", fontsize=8, ha=\"center\", va=\"center\")\n",
    "        axs[i, 0].set(ylabel=f\"{ds_train.decode_one(i)}\")\n",
    "    fig.tight_layout()\n",
    "    visualise_logit(tmp_logits[0, seq_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_in = model.W_in[0, :, :]\n",
    "\n",
    "E_base = torch.concat([model.W_E[:9], model.W_pos[:10]], dim=0)\n",
    "E = E_base @ model.OV[0, :].AB\n",
    "E = torch.concat([E, E_base.unsqueeze(0)], dim=0)\n",
    "E = torch.concat([E, E.sum(0, keepdim=True)], dim=0)\n",
    "E = normalize(E)\n",
    "\n",
    "# sim = cosine_sim(V_in.T, E)\n",
    "sim = torch.tensordot(V_in.T, E, dims=([-1], [-1]))\n",
    "\n",
    "print(f\"{sim.shape=}\")\n",
    "with plt.style.context(\"default\"):\n",
    "    fig, axs = plt.subplots(ncols=len(E), figsize=(20, 100))\n",
    "    order_ = torch.argsort(torch.amax(sim, dim=(-2, -1)), descending=True)[:200]\n",
    "    for i, ax in enumerate(axs):\n",
    "        sns.heatmap(\n",
    "            sim[order_, i].cpu(),\n",
    "            cmap=\"RdBu\",\n",
    "            center=0,\n",
    "            square=True,\n",
    "            ax=ax,\n",
    "            # vmin=0,\n",
    "            yticklabels=order_.cpu().numpy(),\n",
    "            cbar=True,\n",
    "        )\n",
    "        ax.set(title=f\"Head {i}\")\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_circuit = model.W_out[0] @ model.W_U\n",
    "\n",
    "effects = out_circuit.amax(dim=-1) - out_circuit.amin(dim=-1)\n",
    "neurons = torch.argsort(effects, descending=True)[:30]\n",
    "\n",
    "per_row = 10\n",
    "scale = 0.25\n",
    "vmin = out_circuit[neurons].min().item()\n",
    "vmax = out_circuit[neurons].max().item()\n",
    "\n",
    "with plt.style.context(\"no-latex\"):\n",
    "    fig, axs = plt.subplots(\n",
    "        ncols=2 * per_row,\n",
    "        nrows=math.ceil(len(neurons) / per_row),\n",
    "        figsize=(4 * scale * per_row, 4 * scale * math.ceil(len(neurons) / per_row)),\n",
    "        width_ratios=[3, 1] * per_row,\n",
    "        squeeze=False,\n",
    "    )\n",
    "    for i, neuron in enumerate(neurons):\n",
    "        ax1 = axs[i // per_row, 2 * (i % per_row)]\n",
    "        ax2 = axs[i // per_row, 2 * (i % per_row) + 1]\n",
    "        ax1.set(title=f\"{neuron}\")\n",
    "\n",
    "        visualise_logit(\n",
    "            out_circuit[neuron],\n",
    "            ax1,\n",
    "            ax2,\n",
    "            # vmin,\n",
    "            vmax=out_circuit[neuron].max().item(),\n",
    "            center=out_circuit[neuron].max().item() - 1,\n",
    "        )\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_acts = cache[\"mlp_mid\", 0].view(-1, model.cfg.d_mlp)\n",
    "\n",
    "threshold = torch.quantile(neuron_acts.flatten()[::100], 0.99)\n",
    "# threshold = 1e-2\n",
    "active = neuron_acts > threshold\n",
    "activity = active.sum(0)\n",
    "\n",
    "print(threshold)\n",
    "print(torch.argsort(activity, descending=True)[:100])\n",
    "# print(torch.where(active))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "sns.ecdfplot(activity.flatten().cpu().numpy(), ax=ax)\n",
    "# ax.set(xscale=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_neuron_input_output(neuron: int):\n",
    "    fig, axs = plt.subplots(\n",
    "        ncols=8,\n",
    "        nrows=1,\n",
    "        figsize=(8, 2),\n",
    "        gridspec_kw={\"width_ratios\": [1, 1 / 9] * 3 + [1, 1 / 3]},\n",
    "    )\n",
    "\n",
    "    E_base = torch.concat([model.W_E[:9], model.W_pos[:10]], dim=0)\n",
    "    in_circuits = [\n",
    "        E_base @ model.W_in[0, :, neuron] + model.b_in[0, neuron],\n",
    "        E_base @ model.OV[0, 0, :].AB @ model.W_in[0, :, neuron]\n",
    "        + model.b_in[0, neuron],\n",
    "        E_base @ model.OV[0, 1, :].AB @ model.W_in[0, :, neuron]\n",
    "        + model.b_in[0, neuron],\n",
    "    ]\n",
    "    vmin = min(in_circuit.min().item() for in_circuit in in_circuits)\n",
    "    vmax = max(in_circuit.max().item() for in_circuit in in_circuits)\n",
    "\n",
    "    for i, in_circuit in enumerate(in_circuits):\n",
    "        board = in_circuit[:9].reshape(3, 3)\n",
    "        pos = in_circuit[9:].unsqueeze(-1)\n",
    "        sns.heatmap(\n",
    "            board.cpu().numpy(),\n",
    "            ax=axs[i * 2],\n",
    "            cmap=\"RdBu\",\n",
    "            # center=0,\n",
    "            square=True,\n",
    "            cbar=False,\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "            xticklabels=[],\n",
    "            yticklabels=[],\n",
    "        )\n",
    "        sns.heatmap(\n",
    "            pos.cpu().numpy(),\n",
    "            ax=axs[i * 2 + 1],\n",
    "            cmap=\"RdBu\",\n",
    "            # center=0,\n",
    "            square=True,\n",
    "            cbar=False,\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "            yticklabels=[],\n",
    "            xticklabels=[],\n",
    "        )\n",
    "\n",
    "    out_circuit = model.W_out[0, neuron] @ model.W_U\n",
    "    visualise_logit(out_circuit, axs[-2], axs[-1])\n",
    "\n",
    "\n",
    "visualise_neuron_input_output(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = normalize(model.W_pos[:10])\n",
    "P0 = P @ model.OV[0, 0, :].AB\n",
    "P1 = P @ model.OV[0, 1, :].AB\n",
    "\n",
    "sim = P @ model.W_in[0]\n",
    "sim0 = P0 @ model.W_in[0]\n",
    "sim1 = P1 @ model.W_in[0]\n",
    "# sim = cosine_sim(P, model.W_in[0].T)\n",
    "# sim0 = cosine_sim(P0, model.W_in[0].T)\n",
    "# sim1 = cosine_sim(P1, model.W_in[0].T)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "sns.histplot(sim.flatten().cpu().numpy(), ax=ax, label=\"P\")\n",
    "sns.histplot(sim0.flatten().cpu().numpy(), ax=ax, label=\"P0\")\n",
    "sns.histplot(sim1.flatten().cpu().numpy(), ax=ax, label=\"P1\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p_logits, q_logits):\n",
    "    return torch.where(\n",
    "        (p_logits != float(\"-inf\")) & (q_logits == float(\"-inf\")),\n",
    "        float(\"nan\"),\n",
    "        torch.where(\n",
    "            p_logits == float(\"-inf\"),\n",
    "            0,\n",
    "            p_logits.softmax(-1)\n",
    "            * (p_logits.log_softmax(-1) - q_logits.log_softmax(-1)),\n",
    "        ),\n",
    "    ).sum(dim=-1)\n",
    "\n",
    "\n",
    "def measure_neuron_effect(neuron: int):\n",
    "    def neuron_hook(value, hook):\n",
    "        value[:, :, neuron] = 0\n",
    "        return value\n",
    "\n",
    "    patched_logits = model.run_with_hooks(\n",
    "        focus_games[:, :-1],\n",
    "        fwd_hooks=[(utils.get_act_name(\"mlp_mid\", 0), neuron_hook)],\n",
    "    )\n",
    "    loss = (\n",
    "        kl_divergence(patched_logits, logits).mean()\n",
    "        + kl_divergence(logits, patched_logits).mean()\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "\n",
    "data = []\n",
    "for neuron in tqdm(range(model.cfg.d_mlp)):\n",
    "    data.append({\"neuron\": neuron, \"loss\": measure_neuron_effect(neuron).item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out/model/exp21/neurons_40000.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame({\"neuron\": int(k), \"importance\": -v} for k, v in data.items())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "sns.ecdfplot(data=df, x=\"importance\", ax=ax)\n",
    "ax.set_xscale(\"symlog\", linthresh=1e-3)\n",
    "ax.set(xlabel=\"Increaes in loss\")\n",
    "\n",
    "# draw 90% line\n",
    "ax.axvline(df[\"importance\"].quantile(0.9), color=\"k\", linestyle=\"--\")\n",
    "ax.text(\n",
    "    df[\"importance\"].quantile(0.9) + 1e-4,\n",
    "    0.75,\n",
    "    f\"{df['importance'].quantile(0.9):.2e}\",\n",
    "    color=\"k\",\n",
    ")\n",
    "\n",
    "fig.savefig(\"out/figs/neuron_importance.pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "# df = df.sort_values(\"importance\", ascending=False)\n",
    "# df[\"importance\"] -= df[\"importance\"].min()\n",
    "# df[\"cum_importance\"] = df[\"importance\"].cumsum()\n",
    "# df[\"cum_importance\"] /= df[\"cum_importance\"].max()\n",
    "\n",
    "# top_neurons = df[df[\"cum_importance\"] < 0.99]\n",
    "# len(top_neurons)\n",
    "\n",
    "# print(df[\"neuron\"].to_list().index(505))\n",
    "# display(top_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_neuron_input_output(423)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocab = [12, 0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "E = model.W_E[target_vocab]\n",
    "P = model.W_pos[:10]\n",
    "\n",
    "\n",
    "def labels_for(A):\n",
    "    if A is E:\n",
    "        labels = [ds_train.decode_one(i) for i in target_vocab]\n",
    "        return [l if isinstance(l, str) else f\"$E_{l}$\" for l in labels]\n",
    "    elif A is P:\n",
    "        return [f\"$P_{i}$\" for i in range(len(A))]\n",
    "    assert False\n",
    "\n",
    "\n",
    "def title_for(A):\n",
    "    if A is E:\n",
    "        return \"W_E\"\n",
    "    elif A is P:\n",
    "        return \"W_P\"\n",
    "    raise ValueError(\"Unknown matrix\")\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(9, 5))\n",
    "subfigs = fig.subfigures(1, 3, width_ratios=[1, 1, 0.07])\n",
    "\n",
    "for i, subfig in enumerate(subfigs[:2]):\n",
    "    subfig.suptitle(f\"Head {i}\", fontsize=\"xx-large\")\n",
    "    subfig.supxlabel(\"Key\", fontsize=\"x-large\")\n",
    "    subfig.supylabel(\"Query\", fontsize=\"x-large\")\n",
    "    axs = subfig.subplots(2, 2)\n",
    "    for j, (A, B) in enumerate([(E, E), (E, P), (P, E), (P, P)]):\n",
    "        sim = A @ model.QK[0, i].AB @ B.T\n",
    "        if A is P and B is P:\n",
    "            sim = torch.tril(sim)\n",
    "        if A is E:\n",
    "            sim[0, 1:] = 0\n",
    "        sns.heatmap(\n",
    "            sim.cpu(),\n",
    "            ax=axs.flat[j],\n",
    "            vmax=0.06,\n",
    "            center=0,\n",
    "            yticklabels=labels_for(A),\n",
    "            xticklabels=labels_for(B),\n",
    "            cmap=\"RdBu\",\n",
    "            square=True,\n",
    "            cbar=False,\n",
    "            linewidth=0.5,\n",
    "            linecolor=\"black\",\n",
    "            clip_on=False,\n",
    "        )\n",
    "        # axs.flat[j].set(title=f\"${title_for(A)}^T W_Q^T W_K {title_for(B)}$\")\n",
    "\n",
    "cbar_ax = subfigs[-1].add_subplot()\n",
    "im = cm.ScalarMappable(cmap=\"RdBu\", norm=colors.Normalize(vmin=-0.06, vmax=0.06))\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "# fig.tight_layout()\n",
    "\n",
    "# fig.savefig(\"out/figs/attention.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = torch.concat([model.W_E[:9], model.W_pos[:10]], dim=0)\n",
    "print(f\"{E.shape=}\")\n",
    "# P = model.W_pos[:10]\n",
    "\n",
    "# sim = cosine_sim(E, E)\n",
    "sim = torch.tensordot(E, E, dims=([-1], [-1]))\n",
    "sim.fill_diagonal_(0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "labels = [ds_train.decode_one(i) for i in range(9)] + list(range(10))\n",
    "sns.heatmap(\n",
    "    sim.cpu(),\n",
    "    cmap=\"RdBu\",\n",
    "    center=0,\n",
    "    square=True,\n",
    "    ax=ax,\n",
    "    yticklabels=labels,\n",
    "    xticklabels=labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = torch.tensordot(model.W_out[0], model.W_U.T, dims=([-1], [-1]))\n",
    "# sim = cosine_sim(model.W_out[0], model.W_U.T)\n",
    "sim = sim - torch.amin(sim, dim=-1, keepdim=True)\n",
    "order_ = torch.argsort(torch.amax(sim.abs(), dim=(-1)), descending=True)\n",
    "sim = sim[order_]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 100))\n",
    "sns.heatmap(\n",
    "    sim.cpu(),\n",
    "    cmap=\"RdBu\",\n",
    "    center=0,\n",
    "    square=True,\n",
    "    ax=ax,\n",
    "    yticklabels=order_.tolist(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qk_solbol_indices(head_idx):\n",
    "    E = model.W_E[[0, 1, 2, 3, 4, 5, 6, 7, 8]]\n",
    "    P = model.W_pos[[1, 2, 3, 4, 5, 6, 7, 8, 9]]\n",
    "    EP = rearrange(E, \"v d -> 1 v d\") + rearrange(P, \"p d -> p 1 d\")\n",
    "    EP = normalize(EP, dim=-1)\n",
    "    EP_Q = EP @ model.W_Q[0, head_idx] + model.b_Q[0, head_idx]\n",
    "    EP_K = EP @ model.W_K[0, head_idx] + model.b_K[0, head_idx]\n",
    "    A = torch.tensordot(EP_Q, EP_K, dims=([-1], [-1]))\n",
    "    casual_mask = torch.tril(torch.ones(len(P), len(P)))\n",
    "    casual_mask = rearrange(casual_mask, \"i j -> i 1 j 1\")\n",
    "    A.masked_fill_(casual_mask == 0, 0)\n",
    "\n",
    "    g0 = torch.mean(A)\n",
    "\n",
    "    def g1_fn(i):\n",
    "        dims = [j for j in range(A.ndim) if j != i]\n",
    "        return torch.mean(A, dim=dims) - g0\n",
    "\n",
    "    g1 = [g1_fn(i) for i in range(A.ndim)]\n",
    "\n",
    "    def g2_fn(i, j):\n",
    "        assert i < j\n",
    "        dims = [k for k in range(A.ndim) if k != i and k != j]\n",
    "        return torch.mean(A, dim=dims) - g0 - g1[i].unsqueeze(-1) - g1[j]\n",
    "\n",
    "    g2 = [[g2_fn(i, j) for i in range(j)] for j in range(A.ndim)]\n",
    "\n",
    "    v = torch.var(A, unbiased=False).item()\n",
    "    v1 = [torch.var(g, unbiased=False).item() for g in g1]\n",
    "    v2 = [[torch.var(g, unbiased=False).item() for g in gs] for gs in g2]\n",
    "\n",
    "    sobol = {\n",
    "        \"$p_q$\": v1[0] / v,\n",
    "        \"$t_q$\": v1[1] / v,\n",
    "        \"$p_k$\": v1[2] / v,\n",
    "        \"$t_k$\": v1[3] / v,\n",
    "        \"$t_q, p_q$\": v2[1][0] / v,\n",
    "        \"$p_q, p_k$\": v2[2][0] / v,\n",
    "        \"$t_q, p_k$\": v2[2][1] / v,\n",
    "        \"$p_q, t_k$\": v2[3][0] / v,\n",
    "        \"$t_q, t_k$\": v2[3][1] / v,\n",
    "        \"$t_k, p_k$\": v2[3][2] / v,\n",
    "    }\n",
    "    sobol[\"Higher\\norder\"] = 1 - sum(sobol.values())\n",
    "    return sobol\n",
    "\n",
    "\n",
    "data = []\n",
    "for head_idx in range(model.cfg.n_heads):\n",
    "    data.append({\"Head\": head_idx, **qk_solbol_indices(head_idx)})\n",
    "df = pd.DataFrame(data).melt(id_vars=\"Head\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "order = df[df[\"Head\"] == 0].sort_values(\"value\", ascending=False)[\"variable\"]\n",
    "order = order[:5]\n",
    "sns.barplot(data=df, x=\"variable\", y=\"value\", hue=\"Head\", ax=ax, order=order)\n",
    "ax.set(\n",
    "    ylabel=\"Variance of $a^{(i)}(t_q, p_q, t_k, p_k)$ explained\",\n",
    "    xlabel=\"\",\n",
    ")\n",
    "# fig.savefig(\"out/figs/sobol.pdf\", bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tic-tac-gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
