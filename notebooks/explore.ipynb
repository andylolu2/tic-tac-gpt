{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from lightning import fabric\n",
    "from transformer_lens import (\n",
    "    HookedTransformer,\n",
    "    HookedTransformerConfig,\n",
    "    FactoredMatrix,\n",
    "    ActivationCache,\n",
    ")\n",
    "from transformer_lens import utils\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier, MultiOutputRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import log_loss\n",
    "from einops import einsum, rearrange, unpack\n",
    "\n",
    "from tic_tac_gpt.data import TicTacToeDataset, TicTacToeState, tensor_to_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "torch.set_default_device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = Path(\"out/model/exp13\")\n",
    "\n",
    "with open(checkpoint_dir / \"config.pkl\", \"rb\") as f:\n",
    "    config: HookedTransformerConfig = pickle.load(f)\n",
    "model = HookedTransformer(config)\n",
    "F = fabric.Fabric(precision=\"16-mixed\")\n",
    "state_dict = F.load(checkpoint_dir / \"model.pt\")\n",
    "model.load_and_process_state_dict(state_dict)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = TicTacToeDataset(Path(\"out/dataset/50_50/train.jsonl\"))\n",
    "ds_test = TicTacToeDataset(Path(\"out/dataset/50_50/test.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x,) = random.choice(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_logit(logit, ax1=None, ax2=None):\n",
    "    probs = logit\n",
    "    board = [[0] * 3 for _ in range(3)]\n",
    "    special = {}\n",
    "    for i, p in enumerate(probs.tolist()):\n",
    "        move = TicTacToeDataset.decode_one(i)\n",
    "        if isinstance(move, int):\n",
    "            board[move // 3][move % 3] = p\n",
    "        else:\n",
    "            special[move] = p\n",
    "\n",
    "    if ax1 is None or ax2 is None:\n",
    "        fig, axs = plt.subplots(\n",
    "            ncols=2, figsize=(6, 2), gridspec_kw={\"width_ratios\": [1.5, 1]}\n",
    "        )\n",
    "    else:\n",
    "        axs = (ax1, ax2)\n",
    "    sns.heatmap(\n",
    "        board, annot=True, fmt=\".2f\", ax=axs[0], cmap=\"RdBu\", center=0, square=True\n",
    "    )\n",
    "    sns.barplot(x=list(special.keys()), y=list(special.values()), ax=axs[1])\n",
    "    axs[1].set(title=\"Special moves\")\n",
    "\n",
    "\n",
    "def visualise_board(state):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(state.board, cmap=\"gray\", aspect=\"equal\", vmin=0, vmax=2)\n",
    "    ax.set(xticks=[], yticks=[], title=\"Board\")\n",
    "    return fig\n",
    "\n",
    "\n",
    "print(tensor_to_state(x))\n",
    "visualise_logit(model(x)[0, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_games = ds_train[torch.randperm(len(ds_train))[:2048]][0]\n",
    "focus_games_test = ds_test[torch.randperm(len(ds_test))[:2048]][0]\n",
    "focus_games.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, cache = model.run_with_cache(focus_games[:, :-1])\n",
    "cache.compute_head_results()\n",
    "logits_test, cache_test = model.run_with_cache(focus_games_test[:, :-1])\n",
    "cache_test.compute_head_results()\n",
    "print(cache.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_states(game: torch.Tensor):\n",
    "    for i in range(1, game.shape[0] + 1):\n",
    "        state = tensor_to_state(game[:i])\n",
    "        yield state\n",
    "        if state.result != \"in_progress\":\n",
    "            break\n",
    "\n",
    "\n",
    "def board_to_tensor(board):\n",
    "    return torch.tensor(board).flatten(-2, -1)\n",
    "\n",
    "\n",
    "board_states = extract_states(focus_games[7])\n",
    "for s in board_states:\n",
    "    print(s)\n",
    "    print(board_to_tensor(s.board))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_layer = 0\n",
    "\n",
    "\n",
    "def invert(x):\n",
    "    y = x.clone()\n",
    "    y[x == 1] = 2\n",
    "    y[x == 2] = 1\n",
    "    return y\n",
    "\n",
    "\n",
    "def remap(x):\n",
    "    y = x.clone()\n",
    "    y[x == 0] = 0\n",
    "    y[x == 1] = 1\n",
    "    y[x == 2] = 2\n",
    "    return y\n",
    "\n",
    "\n",
    "def extract_XY(games, cache):\n",
    "    activations = cache[\"resid_post\", probe_layer]\n",
    "    # activations = cache[\"attn_out\", probe_layer]\n",
    "    # activations = cache[\"z\", probe_layer, \"attn\"][:, :, 2, :]\n",
    "    X, Y = [], []\n",
    "    for game, game_activation in zip(games, activations):\n",
    "        for i, (state, activation) in enumerate(\n",
    "            zip(extract_states(game), game_activation)\n",
    "        ):\n",
    "            X.append(activation)\n",
    "            target = board_to_tensor(state.board)\n",
    "            if i % 2 == 0:\n",
    "                target = invert(target)\n",
    "            # target = remap(target)\n",
    "            Y.append(target)\n",
    "            # Y.append(torch.nn.functional.one_hot(target, 3))\n",
    "    X, Y = torch.stack(X), torch.stack(Y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "X, Y = extract_XY(focus_games, cache)\n",
    "X_test, Y_test = extract_XY(focus_games_test, cache_test)\n",
    "X.shape, Y.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np = X.cpu().numpy()\n",
    "Y_np = Y.cpu().numpy()\n",
    "# Y_np = Y.flatten(-2, -1).cpu().numpy()\n",
    "X_test_np = X_test.cpu().numpy()\n",
    "Y_test_np = Y_test.cpu().numpy()\n",
    "# Y_test_np = Y_test.flatten(-2, -1).cpu().numpy()\n",
    "\n",
    "regressor = MultiOutputRegressor(LinearRegression())\n",
    "regressor.fit(X_np, Y_np)\n",
    "print(regressor.score(X_np, Y_np), regressor.score(X_test_np, Y_test_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np = X.cpu().numpy()\n",
    "Y_np = Y.cpu().numpy()\n",
    "X_test_np = X_test.cpu().numpy()\n",
    "Y_test_np = Y_test.cpu().numpy()\n",
    "\n",
    "classifier = MultiOutputClassifier(LogisticRegression(max_iter=1000))\n",
    "classifier.fit(X_np, Y_np)\n",
    "\n",
    "Y_pred = np.array(classifier.predict_proba(X_np)).transpose(1, 0, 2)\n",
    "Y_test_pred = np.array(classifier.predict_proba(X_test_np)).transpose(1, 0, 2)\n",
    "print(classifier.score(X_np, Y_np), classifier.score(X_test_np, Y_test_np))\n",
    "print(\n",
    "    log_loss(Y_np.reshape(-1), Y_pred.reshape(-1, Y_pred.shape[-1])),\n",
    "    log_loss(Y_test_np.reshape(-1), Y_test_pred.reshape(-1, Y_test_pred.shape[-1])),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attn(x, pattern):\n",
    "    \"\"\"Input shape (nh s s)\"\"\"\n",
    "    fig, axs = plt.subplots(ncols=pattern.shape[0], figsize=(pattern.shape[0] * 4, 4))\n",
    "    for i, ax in enumerate(axs):\n",
    "        p = pattern[i]\n",
    "        # p /= np.max(p, axis=-1, keepdims=True)\n",
    "        sns.heatmap(\n",
    "            p,\n",
    "            ax=ax,\n",
    "            cmap=\"viridis\",\n",
    "            square=True,\n",
    "            annot=True,\n",
    "            annot_kws={\"fontsize\": 6},\n",
    "            xticklabels=x,\n",
    "            yticklabels=x,\n",
    "        )\n",
    "        ax.set(title=f\"Head {i}\")\n",
    "    fig.tight_layout()\n",
    "    # return fig\n",
    "\n",
    "\n",
    "idx = random.randint(0, len(focus_games))\n",
    "game = focus_games[idx]\n",
    "# game = torch.tensor([ds_train.bos_token, 1, 5, 4, 8, 7])\n",
    "print(tensor_to_state(game))\n",
    "\n",
    "_, tmp_cache = model.run_with_cache(game)\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    plot_attn(\n",
    "        TicTacToeDataset.decode(game),\n",
    "        tmp_cache[\"pattern\", layer][0].cpu().numpy(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(x: torch.Tensor, y: torch.Tensor):\n",
    "    x = torch.nn.functional.normalize(x, dim=-1)\n",
    "    y = torch.nn.functional.normalize(y, dim=-1)\n",
    "    return torch.tensordot(x, y, dims=([-1], [-1]))\n",
    "\n",
    "\n",
    "x = torch.randn(4, 5)\n",
    "y = torch.randn(3, 5)\n",
    "assert (cosine_sim(x, y) == cosine_sim(y, x).T).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_idx = 1\n",
    "\n",
    "seq_activations = [[] for _ in range(ds_train.max_seq_len - 1)]\n",
    "for activations, game in zip(cache[\"result\", 0, \"attn\"], focus_games_test):\n",
    "    activations = activations[:, head_idx]\n",
    "    for i, (activation, move) in enumerate(zip(activations, game)):\n",
    "        move = move.item()\n",
    "        if move in (0, 1, 2, 3, 4, 5, 6, 7, 8, ds_train.bos_token):\n",
    "            seq_activations[i].append(activation)\n",
    "seq_activations = [torch.stack(s) for s in seq_activations]\n",
    "print([s.shape for s in seq_activations])\n",
    "\n",
    "data = []\n",
    "for i, activations in enumerate(seq_activations):\n",
    "    singulars = torch.linalg.svdvals(activations).cpu().numpy()\n",
    "    for j, s in enumerate(sorted(singulars, reverse=True)):\n",
    "        data.append({\"idx\": j, \"singular\": s, \"position\": str(i)})\n",
    "\n",
    "ax = sns.lineplot(data=pd.DataFrame(data), x=\"idx\", y=\"singular\", hue=\"position\")\n",
    "# for x in (1, 9, 10, 11, 12, 13, 15, 17, 19, 20):\n",
    "for x in (0, 8, 10, 16, 20, 22, 24, 26, 29, 32, 35, 38):\n",
    "    ax.axvline(x, color=\"black\", linestyle=\"--\", alpha=0.5)\n",
    "ax.set(yscale=\"log\", ylim=(1e-5, None), xlim=(-1, 22))\n",
    "# ax.set(yscale=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cache.keys())\n",
    "\n",
    "head_idx = 1\n",
    "act_Q = cache[\"blocks.0.attn.hook_q\"][:, :, head_idx, :]\n",
    "act_K = cache[\"blocks.0.attn.hook_k\"][:, :, head_idx, :]\n",
    "act_V = cache[\"blocks.0.attn.hook_v\"][:, :, head_idx, :]\n",
    "\n",
    "seq_idx = torch.arange(act_Q.shape[1]).tile((act_Q.shape[0], 1)) % 2 == 0\n",
    "seq_idx = torch.flatten(seq_idx, 0, 1).cpu().numpy()\n",
    "\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(12, 4))\n",
    "for ax, act in zip(axs, (act_Q, act_K, act_V)):\n",
    "    act = torch.flatten(act, 0, 1).cpu().numpy()\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(act)\n",
    "    print(pca.explained_variance_ratio_)\n",
    "    ax.scatter(X_pca[:, 0], X_pca[:, 1], c=seq_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_win_vec = model.W_U[:, 9]\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    x = x - x.mean(-1, keepdim=True)\n",
    "    scale = (x.pow(2).mean(-1, keepdim=True)).sqrt()\n",
    "    return x / scale\n",
    "\n",
    "\n",
    "E_P = model.W_E[:, None, :] + model.W_pos[None, :, :]\n",
    "E_P = normalize(E_P)\n",
    "print(E_P.shape)\n",
    "\n",
    "E = (\n",
    "    0.05 * E_P[12, 0]\n",
    "    + 0.1 * E_P[1, 1]\n",
    "    + 0.3 * E_P[5, 2]\n",
    "    + 0.1 * E_P[4, 3]\n",
    "    + 0.3 * E_P[8, 4]\n",
    "    + 0.1 * E_P[7, 5]\n",
    ")\n",
    "E = E @ model.OV[0, 1].AB\n",
    "\n",
    "cosine_sim(x_win_vec, E), E @ x_win_vec\n",
    "\n",
    "\n",
    "# E_P = model.W_E[:9][:, None, :] + model.W_pos[:10][None, :, :]\n",
    "# E_P = normalize(E_P)\n",
    "# # E_P_E_P = rearrange(E_P, \"e p d -> 1 1 e p d\") + rearrange(E_P, \"e p d -> e p 1 1 d\")\n",
    "# E_P_E_P_E_P = (\n",
    "#     rearrange(E_P, \"e p d -> 1 1 1 1 e p d\")\n",
    "#     + rearrange(E_P, \"e p d -> 1 1 e p 1 1 d\")\n",
    "#     + rearrange(E_P, \"e p d -> e p 1 1 1 1 d\")\n",
    "# )\n",
    "# # E = embeds[None, :, :] + embeds[:, None, :]\n",
    "# E = E_P_E_P_E_P @ model.OV[0, 0].AB\n",
    "\n",
    "# # sim = cosine_sim(E, x_win_vec).abs()\n",
    "# sim = E @ x_win_vec\n",
    "# # sim = rearrange(sim, \"e1 p1 e2 p2 -> (e1 p1) (e2 p2)\")\n",
    "# print(sim.shape)\n",
    "# print(sim[1, 3, 4, 5, 7, 7])\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(20, 10))\n",
    "# sns.heatmap(sim.cpu(), cmap=\"RdBu\", center=0, square=True, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cache.keys())\n",
    "A = cache[\"v\", 0, \"attn\"].flatten(-2, -1)\n",
    "# A = einsum(A, model.W_O[0], \"b s h d, h d d2 -> b s h d2\")\n",
    "# A, B = A[:, :, 0], A[:, :, 3]\n",
    "B = cache[\"resid_pre\", 0]\n",
    "print(A.shape, B.shape, model.W_O[0].shape)\n",
    "\n",
    "seq_idx = 6\n",
    "A = A[:, seq_idx, :]\n",
    "B = B[:, seq_idx, :]\n",
    "\n",
    "U, S, Vh = A.svd()\n",
    "R = torch.linalg.matrix_rank(A)\n",
    "U_B, S_B, Vh_B = B.svd()\n",
    "R_B = torch.linalg.matrix_rank(B)\n",
    "print(U.shape, S.shape, Vh.shape, R)\n",
    "print(U_B.shape, S_B.shape, Vh_B.shape, R_B)\n",
    "\n",
    "sim = cosine_sim(Vh[:R], Vh_B[:R_B])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "sns.heatmap(sim.cpu(), cmap=\"RdBu\", center=0, square=True, ax=ax)\n",
    "# sns.histplot(sim.cpu().numpy().flatten(), bins=100, ax=ax, stat=\"density\")\n",
    "# sns.lineplot(x=range(1, len(S) + 1), y=S.cpu().numpy(), ax=ax)\n",
    "# sns.lineplot(x=range(1, len(S_resid) + 1), y=S_resid.cpu().numpy(), ax=ax)\n",
    "# ax.set(yscale=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_out_0 = cache[\"attn_out\", 0].flatten(0, 1)\n",
    "V_base = model.W_E_pos\n",
    "\n",
    "U_out_0, S_out_0, Vh_out_0 = torch.linalg.svd(V_out_0, full_matrices=False)\n",
    "U_base, S_base, Vh_base = torch.linalg.svd(V_base, full_matrices=False)\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(10, 4))\n",
    "sns.lineplot(x=range(len(S_out_0)), y=S_out_0.cpu(), ax=axs[0])\n",
    "sns.lineplot(x=range(len(S_base)), y=S_base.cpu(), ax=axs[1])\n",
    "axs[0].axvline(85, color=\"black\", linestyle=\"--\", alpha=0.5)\n",
    "axs[0].set(yscale=\"log\")\n",
    "axs[1].set(yscale=\"log\")\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "# V_base = V_base / torch.linalg.norm(V_base, dim=-1, keepdim=True)\n",
    "# V_out_0 = V_out_0 / torch.linalg.norm(V_out_0, dim=-1, keepdim=True)\n",
    "\n",
    "# sim_out_0_out_0 = V_out_0 @ V_out_0.transpose(-2, -1)\n",
    "sim_out_0_base = cosine_sim(V_out_0, V_base)\n",
    "sim_base_base = cosine_sim(V_base, V_base)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# sns.histplot(sim_out_0_out_0.flatten().cpu(), ax=ax, label=\"out_0 vs out_0\")\n",
    "# sns.histplot(sim_out_0_base.flatten().cpu(), ax=ax, label=\"out_0 vs base\")\n",
    "sns.histplot(sim_base_base.flatten().cpu(), ax=ax, label=\"base vs base\", stat=\"density\")\n",
    "# ax.set(yscale=\"log\")\n",
    "# ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_1 = torch.tensor([[ds_train.bos_token, 0, 5, 1, 8, 2]])\n",
    "game_2 = torch.tensor([[ds_train.bos_token, 0, 5, 1, 8, 2]])\n",
    "print(tensor_to_state(game_1[0]))\n",
    "print(tensor_to_state(game_2[0]))\n",
    "\n",
    "clean_logits, clean_cache = model.run_with_cache(game_1)\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(game_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_head_vector(corrupted_head_vector, hook, head_index):\n",
    "    corrupted_head_vector[:, -2, head_index, :] = corrupted_cache[hook.name][\n",
    "        :, -1, head_index, :\n",
    "    ]\n",
    "    return corrupted_head_vector\n",
    "\n",
    "\n",
    "def logit_diff(patched_logits):\n",
    "    # terminate_idx = [9, 10, 11]\n",
    "    # other_idx = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "    terminate_idx = [10]\n",
    "    other_idx = [9]\n",
    "    seq_idx = -2\n",
    "    diff_patched = patched_logits[:, seq_idx, terminate_idx].mean(-1) - patched_logits[\n",
    "        :, seq_idx, other_idx\n",
    "    ].mean(-1)\n",
    "    diff_clean = clean_logits[:, seq_idx, terminate_idx].mean(-1) - clean_logits[\n",
    "        :, seq_idx, other_idx\n",
    "    ].mean(-1)\n",
    "    rel_change = diff_patched  # - diff_clean\n",
    "    return rel_change.mean()\n",
    "\n",
    "\n",
    "patched_residual_stream_diff = torch.zeros(\n",
    "    model.cfg.n_layers, model.cfg.n_heads, game_1.shape[1]\n",
    ")\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    for head_index in range(model.cfg.n_heads):\n",
    "        for seq_idx in range(game_1.shape[1]):\n",
    "            hook_fn = partial(patch_head_vector, head_index=head_index)\n",
    "            patched_logits = model.run_with_hooks(\n",
    "                game_1,\n",
    "                fwd_hooks=[(utils.get_act_name(\"v\", layer, \"attn\"), hook_fn)],\n",
    "                return_type=\"logits\",\n",
    "            )\n",
    "            patched_logit_diff = logit_diff(patched_logits)\n",
    "            patched_residual_stream_diff[\n",
    "                layer, head_index, seq_idx\n",
    "            ] = patched_logit_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=model.cfg.n_heads, figsize=(8, 8))\n",
    "_min = patched_residual_stream_diff.min().item()\n",
    "_max = patched_residual_stream_diff.max().item()\n",
    "for i, ax in enumerate(axs):\n",
    "    sns.heatmap(\n",
    "        patched_residual_stream_diff[:, i].cpu(),\n",
    "        ax=ax,\n",
    "        cmap=\"RdBu\",\n",
    "        square=True,\n",
    "        center=0,\n",
    "        vmin=_min,\n",
    "        vmax=_max,\n",
    "    )\n",
    "    ax.set(\n",
    "        xlabel=\"Seq\",\n",
    "        ylabel=\"Layer\",\n",
    "        xticklabels=ds_train.decode(game_1[0]),\n",
    "    )\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_idx = random.randint(0, 2048)\n",
    "# game_tensor = focus_games[game_idx]\n",
    "game_tensor = torch.tensor([ds_train.bos_token, 6, 0, 4, 1, 5, 2])\n",
    "# game_tensor = torch.tensor([ds_train.bos_token, 3, 6, 1, 5, 4, 8, 7])\n",
    "game = tensor_to_state(game_tensor)\n",
    "\n",
    "print(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_idx = 1\n",
    "s = len(game) + 1\n",
    "\n",
    "\n",
    "def logit_contribution(activation):\n",
    "    return torch.matmul(activation, model.W_U[:, out_idx])\n",
    "\n",
    "\n",
    "head_results = cache.stack_head_results()\n",
    "head_results = cache.apply_ln_to_stack(head_results)\n",
    "head_results = rearrange(\n",
    "    head_results, \"(l nh) b s d -> b s l nh d\", l=model.cfg.n_layers\n",
    ")\n",
    "\n",
    "out = torch.zeros(model.cfg.n_layers, model.cfg.n_heads, s)\n",
    "for layer_idx in range(model.cfg.n_layers):\n",
    "    for head_idx in range(model.cfg.n_heads):\n",
    "        for seq_idx in range(s):\n",
    "            out[layer_idx, head_idx, seq_idx] = logit_contribution(\n",
    "                head_results[game_idx, seq_idx, layer_idx, head_idx]\n",
    "            )\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    ncols=3,\n",
    "    nrows=s,\n",
    "    figsize=(10, 18),\n",
    "    gridspec_kw={\"width_ratios\": [3, 1, 3]},\n",
    ")\n",
    "# v_min = out.min().item()\n",
    "# v_max = out.max().item()\n",
    "for seq_idx in range(s):\n",
    "    visualise_logit(logits[game_idx, seq_idx], axs[seq_idx, 0], axs[seq_idx, 1])\n",
    "    sns.heatmap(\n",
    "        out[:, :, seq_idx].cpu(),\n",
    "        ax=axs[seq_idx, 2],\n",
    "        cmap=\"RdBu\",\n",
    "        center=0,\n",
    "        square=True,\n",
    "    )\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_idx = len(game) - 2\n",
    "seq_idx = -2\n",
    "\n",
    "\n",
    "def logit_contribution(activation, out_idx):\n",
    "    return torch.matmul(activation, model.W_U[:, out_idx])\n",
    "\n",
    "\n",
    "tmp_logits, tmp_cache = model.run_with_cache(game_tensor)\n",
    "all_acts = tmp_cache.get_full_resid_decomposition(pos_slice=seq_idx, apply_ln=True)\n",
    "# all_acts = tmp_cache.apply_ln_to_stack(all_acts, pos_slice=seq_idx)\n",
    "all_acts = all_acts[:, 0, :]\n",
    "\n",
    "heads, neurons, emb, pos, bias = unpack(\n",
    "    all_acts,\n",
    "    [\n",
    "        [model.cfg.n_layers, model.cfg.n_heads],\n",
    "        [model.cfg.n_layers, model.cfg.d_mlp],\n",
    "        [1],\n",
    "        [1],\n",
    "        [1],\n",
    "    ],\n",
    "    \"* d\",\n",
    ")\n",
    "print(game)\n",
    "for name, act in {\"emb\": emb, \"pos\": pos, \"bias\": bias}.items():\n",
    "    logits_contributions = [\n",
    "        logit_contribution(act, i).item() for i in range(model.cfg.d_vocab_out)\n",
    "    ]\n",
    "    print(f\"{name:>4}: {' '.join(f'{x:>5.2f}' for x in logits_contributions)}\")\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    ncols=2,\n",
    "    nrows=model.cfg.d_vocab_out,\n",
    "    figsize=(12, 1 * model.cfg.d_vocab_out),\n",
    "    gridspec_kw={\"width_ratios\": [1, 10]},\n",
    "    sharey=\"row\",\n",
    "    sharex=\"col\",\n",
    ")\n",
    "for i in range(model.cfg.d_vocab_out):\n",
    "    heads_logits = logit_contribution(heads, i).flatten()\n",
    "    sns.scatterplot(\n",
    "        x=np.arange(heads_logits.numel()), y=heads_logits.cpu().numpy(), ax=axs[i, 0]\n",
    "    )\n",
    "    neurons_logits = logit_contribution(neurons, i).flatten()\n",
    "    sns.scatterplot(\n",
    "        x=np.arange(neurons_logits.numel()),\n",
    "        y=neurons_logits.cpu().numpy(),\n",
    "        ax=axs[i, 1],\n",
    "    )\n",
    "    # label outliers\n",
    "    for x, y in enumerate(neurons_logits):\n",
    "        if y.abs() > 0.3:\n",
    "            axs[i, 1].text(x, y, f\"{x}\", fontsize=8, ha=\"center\", va=\"center\")\n",
    "    axs[i, 0].set(ylabel=f\"{ds_train.decode_one(i)}\")\n",
    "fig.tight_layout()\n",
    "visualise_logit(tmp_logits[0, seq_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_in = model.W_in[0, :, :]\n",
    "\n",
    "E = model.W_E[:9]\n",
    "E = E @ model.OV[0, :].AB\n",
    "\n",
    "# sim = cosine_sim(V_in.T, E)\n",
    "sim = torch.tensordot(V_in.T, E, dims=([-1], [-1]))\n",
    "\n",
    "print(f\"{sim.shape=}\")\n",
    "fig, axs = plt.subplots(ncols=len(E), figsize=(5, 100), sharey=True)\n",
    "order_ = torch.argsort(torch.amax(sim, dim=(-2, -1)), descending=True)\n",
    "for i, ax in enumerate(axs):\n",
    "    sns.heatmap(\n",
    "        sim[order_, i].cpu(),\n",
    "        cmap=\"RdBu\",\n",
    "        center=0,\n",
    "        square=True,\n",
    "        ax=ax,\n",
    "        vmin=0,\n",
    "        yticklabels=order_.cpu().numpy(),\n",
    "        cbar=True,\n",
    "    )\n",
    "    ax.set(title=f\"Head {i}\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = model.W_E_pos\n",
    "# P = model.W_pos[:10]\n",
    "\n",
    "V1 = E @ model.OV[0, 0].AB\n",
    "V2 = E @ model.OV[0, 1].AB\n",
    "print(f\"{V1.shape=} {V2.shape=}\")\n",
    "\n",
    "sim = cosine_sim(V1, V2)\n",
    "assert (sim == cosine_sim(V2, V1).T).all()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "labels = [ds_train.decode_one(i) for i in range(ds_train.vocab_size)] + list(range(11))\n",
    "sns.heatmap(\n",
    "    sim.cpu(),\n",
    "    cmap=\"RdBu\",\n",
    "    center=0,\n",
    "    square=True,\n",
    "    ax=ax,\n",
    "    yticklabels=labels,\n",
    "    xticklabels=labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = model.W_E_pos\n",
    "\n",
    "sim1 = P @ model.QK[0, 0].AB @ P.T\n",
    "# sim1 = torch.tril(sim1)\n",
    "sim2 = P @ model.QK[0, 1].AB @ P.T\n",
    "# sim2 = torch.tril(sim2)\n",
    "\n",
    "labels = [ds_train.decode_one(i) for i in range(ds_train.vocab_size)] + list(range(11))\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "sns.heatmap(\n",
    "    sim1.cpu(),\n",
    "    cmap=\"RdBu\",\n",
    "    center=0,\n",
    "    square=True,\n",
    "    ax=axs[0],\n",
    "    xticklabels=labels,\n",
    "    yticklabels=labels,\n",
    ")\n",
    "axs[0].set(title=\"Head 0\", xlabel=\"Key position\", ylabel=\"Query position\")\n",
    "sns.heatmap(\n",
    "    sim2.cpu(),\n",
    "    cmap=\"RdBu\",\n",
    "    center=0,\n",
    "    square=True,\n",
    "    ax=axs[1],\n",
    "    xticklabels=labels,\n",
    "    yticklabels=labels,\n",
    ")\n",
    "axs[1].set(title=\"Head 1\", xlabel=\"Key position\", ylabel=\"Query position\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tic-tac-gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
