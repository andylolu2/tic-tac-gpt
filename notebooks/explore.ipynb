{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from lightning import fabric\n",
    "from transformer_lens import (\n",
    "    HookedTransformer,\n",
    "    HookedTransformerConfig,\n",
    "    FactoredMatrix,\n",
    "    ActivationCache,\n",
    ")\n",
    "from transformer_lens import utils\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier, MultiOutputRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import log_loss\n",
    "from einops import einsum\n",
    "\n",
    "from tic_tac_gpt.data import TicTacToeDataset, TicTacToeState, tensor_to_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "torch.set_default_device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = Path(\"out/model/exp6\")\n",
    "\n",
    "with open(checkpoint_dir / \"config.pkl\", \"rb\") as f:\n",
    "    config: HookedTransformerConfig = pickle.load(f)\n",
    "model = HookedTransformer(config)\n",
    "F = fabric.Fabric(precision=\"16-mixed\")\n",
    "state_dict = F.load(checkpoint_dir / \"model.pt\")\n",
    "model.load_and_process_state_dict(state_dict)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = TicTacToeDataset(Path(\"out/dataset/50_50/train.jsonl\"))\n",
    "ds_test = TicTacToeDataset(Path(\"out/dataset/50_50/test.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x,) = random.choice(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_logit(logit):\n",
    "    probs = logit.softmax(0)\n",
    "    board = [[0] * 3 for _ in range(3)]\n",
    "    special = {}\n",
    "    for i, p in enumerate(probs.tolist()):\n",
    "        move = TicTacToeDataset.decode_one(i)\n",
    "        if isinstance(move, int):\n",
    "            board[move // 3][move % 3] = p\n",
    "        else:\n",
    "            special[move] = p\n",
    "\n",
    "    fig, axs = plt.subplots(\n",
    "        ncols=2, figsize=(10, 4), gridspec_kw={\"width_ratios\": [1.5, 1]}\n",
    "    )\n",
    "    m = axs[0].imshow(board, aspect=\"equal\", vmin=0)\n",
    "    axs[0].set(xticks=[], yticks=[], title=\"Logits\")\n",
    "    fig.colorbar(m, ax=axs[0])\n",
    "    sns.barplot(x=list(special.keys()), y=list(special.values()), ax=axs[1])\n",
    "    axs[1].set(title=\"Special moves\", ylim=(0, 1))\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def visualise_board(state):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(state.board, cmap=\"gray\", aspect=\"equal\", vmin=0, vmax=2)\n",
    "    ax.set(xticks=[], yticks=[], title=\"Board\")\n",
    "    return fig\n",
    "\n",
    "\n",
    "print(tensor_to_state(x))\n",
    "visualise_logit(model(x)[0, 3])\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_games = ds_train[torch.randperm(len(ds_train))[:4096]][0]\n",
    "focus_games_test = ds_test[torch.randperm(len(ds_test))[:4096]][0]\n",
    "focus_games.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(focus_games[:, :-1])\n",
    "_, cache_test = model.run_with_cache(focus_games_test[:, :-1])\n",
    "print(cache.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_states(game: torch.Tensor):\n",
    "    for i in range(1, game.shape[0] + 1):\n",
    "        state = tensor_to_state(game[:i])\n",
    "        yield state\n",
    "        if state.result != \"in_progress\":\n",
    "            break\n",
    "\n",
    "\n",
    "def board_to_tensor(board):\n",
    "    return torch.tensor(board).flatten(-2, -1)\n",
    "\n",
    "\n",
    "board_states = extract_states(focus_games[7])\n",
    "for s in board_states:\n",
    "    print(s)\n",
    "    print(board_to_tensor(s.board))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_layer = 0\n",
    "\n",
    "\n",
    "def invert(x):\n",
    "    y = x.clone()\n",
    "    y[x == 1] = 2\n",
    "    y[x == 2] = 1\n",
    "    return y\n",
    "\n",
    "\n",
    "def remap(x):\n",
    "    y = x.clone()\n",
    "    y[x == 0] = 0\n",
    "    y[x == 1] = 1\n",
    "    y[x == 2] = 2\n",
    "    return y\n",
    "\n",
    "\n",
    "def extract_XY(games, cache):\n",
    "    activations = cache[\"resid_post\", probe_layer]\n",
    "    # activations = cache[\"attn_out\", probe_layer]\n",
    "    # activations = cache[\"z\", probe_layer, \"attn\"][:, :, 2, :]\n",
    "    X, Y = [], []\n",
    "    for game, game_activation in zip(games, activations):\n",
    "        for i, (state, activation) in enumerate(\n",
    "            zip(extract_states(game), game_activation)\n",
    "        ):\n",
    "            X.append(activation)\n",
    "            target = board_to_tensor(state.board)\n",
    "            if i % 2 == 0:\n",
    "                target = invert(target)\n",
    "            # target = remap(target)\n",
    "            Y.append(target)\n",
    "            # Y.append(torch.nn.functional.one_hot(target, 3))\n",
    "    X, Y = torch.stack(X), torch.stack(Y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "X, Y = extract_XY(focus_games, cache)\n",
    "X_test, Y_test = extract_XY(focus_games_test, cache_test)\n",
    "X.shape, Y.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np = X.cpu().numpy()\n",
    "Y_np = Y.cpu().numpy()\n",
    "# Y_np = Y.flatten(-2, -1).cpu().numpy()\n",
    "X_test_np = X_test.cpu().numpy()\n",
    "Y_test_np = Y_test.cpu().numpy()\n",
    "# Y_test_np = Y_test.flatten(-2, -1).cpu().numpy()\n",
    "\n",
    "regressor = MultiOutputRegressor(LinearRegression())\n",
    "regressor.fit(X_np, Y_np)\n",
    "print(regressor.score(X_np, Y_np), regressor.score(X_test_np, Y_test_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np = X.cpu().numpy()\n",
    "Y_np = Y.cpu().numpy()\n",
    "X_test_np = X_test.cpu().numpy()\n",
    "Y_test_np = Y_test.cpu().numpy()\n",
    "\n",
    "classifier = MultiOutputClassifier(LogisticRegression(max_iter=1000))\n",
    "classifier.fit(X_np, Y_np)\n",
    "\n",
    "Y_pred = np.array(classifier.predict_proba(X_np)).transpose(1, 0, 2)\n",
    "Y_test_pred = np.array(classifier.predict_proba(X_test_np)).transpose(1, 0, 2)\n",
    "print(classifier.score(X_np, Y_np), classifier.score(X_test_np, Y_test_np))\n",
    "print(\n",
    "    log_loss(Y_np.reshape(-1), Y_pred.reshape(-1, Y_pred.shape[-1])),\n",
    "    log_loss(Y_test_np.reshape(-1), Y_test_pred.reshape(-1, Y_test_pred.shape[-1])),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attn(x, pattern):\n",
    "    \"\"\"Input shape (nh s s)\"\"\"\n",
    "    fig, axs = plt.subplots(ncols=pattern.shape[0], figsize=(pattern.shape[0] * 3, 3))\n",
    "    for i, ax in enumerate(axs):\n",
    "        p = pattern[i]\n",
    "        p /= np.max(p, axis=-1, keepdims=True)\n",
    "        ax.imshow(p, cmap=\"viridis\", aspect=\"equal\")\n",
    "        ax.set(\n",
    "            title=f\"Head {i}\",\n",
    "            xticks=range(len(x)),\n",
    "            xticklabels=x,\n",
    "            yticks=range(len(x)),\n",
    "            yticklabels=x,\n",
    "        )\n",
    "    fig.tight_layout()\n",
    "    # return fig\n",
    "\n",
    "\n",
    "idx = 10\n",
    "print(tensor_to_state(focus_games[idx]))\n",
    "plot_attn(\n",
    "    TicTacToeDataset.decode(focus_games[idx])[:-1],\n",
    "    cache[\"pattern\", 0][idx].cpu().numpy(),\n",
    ")\n",
    "plot_attn(\n",
    "    TicTacToeDataset.decode(focus_games[idx])[:-1],\n",
    "    cache[\"pattern\", 1][idx].cpu().numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(x: torch.Tensor, y: torch.Tensor):\n",
    "    x = x / x.norm(dim=-1, keepdim=True)\n",
    "    y = y / y.norm(dim=-1, keepdim=True)\n",
    "    return x @ y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cache.keys())\n",
    "\n",
    "seq_activations = [[] for _ in range(ds_train.max_seq_len - 1)]\n",
    "for activations, game in zip(cache[\"attn_out\", 0], focus_games_test):\n",
    "    # print(activations.shape)\n",
    "    # activations = activations[:, 0, :]\n",
    "    for i, (activation, move) in enumerate(zip(activations, game)):\n",
    "        move = move.item()\n",
    "        if move in (0, 1, 2, 3, 4, 5, 6, 7, 8, ds_train.bos_token):\n",
    "            seq_activations[i].append(activation)\n",
    "seq_activations = [torch.stack(s) for s in seq_activations]\n",
    "print([s.shape for s in seq_activations])\n",
    "\n",
    "data = []\n",
    "for i, activations in enumerate(seq_activations):\n",
    "    singulars = torch.linalg.svdvals(activations).cpu().numpy()\n",
    "    for j, s in enumerate(sorted(singulars, reverse=True)):\n",
    "        data.append({\"idx\": j, \"singular\": s, \"position\": str(i)})\n",
    "\n",
    "ax = sns.lineplot(data=pd.DataFrame(data), x=\"idx\", y=\"singular\", hue=\"position\")\n",
    "for x in (1, 9, 45, 49, 53, 57, 65, 73, 81, 85):\n",
    "    # for x in (1, 9, 72, 241):\n",
    "    ax.axvline(x, color=\"black\", linestyle=\"--\", alpha=0.5)\n",
    "ax.set(yscale=\"log\", ylim=(1e-9, None))\n",
    "# ax.set(yscale=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cache.keys())\n",
    "\n",
    "head_idx = 3\n",
    "act_Q = cache[\"blocks.0.attn.hook_q\"][:, :, head_idx, :]\n",
    "act_K = cache[\"blocks.0.attn.hook_k\"][:, :, head_idx, :]\n",
    "act_V = cache[\"blocks.0.attn.hook_v\"][:, :, head_idx, :]\n",
    "\n",
    "seq_idx = torch.arange(act_Q.shape[1]).tile((act_Q.shape[0], 1)) % 2 == 0\n",
    "seq_idx = torch.flatten(seq_idx, 0, 1).cpu().numpy()\n",
    "\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(12, 4))\n",
    "for ax, act in zip(axs, (act_Q, act_K, act_V)):\n",
    "    act = torch.flatten(act, 0, 1).cpu().numpy()\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(act)\n",
    "    print(pca.explained_variance_ratio_)\n",
    "    ax.scatter(X_pca[:, 0], X_pca[:, 1], c=seq_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_Q, S_Q, Vh_Q = torch.linalg.svd(model.W_Q[0, 3], full_matrices=False)\n",
    "U_K, S_K, Vh_K = torch.linalg.svd(model.W_K[0, 3], full_matrices=False)\n",
    "# print(U_head.shape, S_head.shape, Vh_head.shape)\n",
    "\n",
    "sim = cosine_sim(Vh_Q, Vh_K)\n",
    "# sim = cosine_sim(model.W_pos, U_Q.T)\n",
    "# sim = cosine_sim(model.W_pos, U_K.T)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "sns.heatmap(sim.cpu(), cmap=\"RdBu\", center=0, square=True, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cache.keys())\n",
    "A = cache[\"v\", 0, \"attn\"].flatten(-2, -1)\n",
    "# A = einsum(A, model.W_O[0], \"b s h d, h d d2 -> b s h d2\")\n",
    "# A, B = A[:, :, 0], A[:, :, 3]\n",
    "B = cache[\"resid_pre\", 0]\n",
    "print(A.shape, B.shape, model.W_O[0].shape)\n",
    "\n",
    "seq_idx = 6\n",
    "A = A[:, seq_idx, :]\n",
    "B = B[:, seq_idx, :]\n",
    "\n",
    "U, S, Vh = A.svd()\n",
    "R = torch.linalg.matrix_rank(A)\n",
    "U_B, S_B, Vh_B = B.svd()\n",
    "R_B = torch.linalg.matrix_rank(B)\n",
    "print(U.shape, S.shape, Vh.shape, R)\n",
    "print(U_B.shape, S_B.shape, Vh_B.shape, R_B)\n",
    "\n",
    "sim = cosine_sim(Vh[:R], Vh_B[:R_B])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "sns.heatmap(sim.cpu(), cmap=\"RdBu\", center=0, square=True, ax=ax)\n",
    "# sns.histplot(sim.cpu().numpy().flatten(), bins=100, ax=ax, stat=\"density\")\n",
    "# sns.lineplot(x=range(1, len(S) + 1), y=S.cpu().numpy(), ax=ax)\n",
    "# sns.lineplot(x=range(1, len(S_resid) + 1), y=S_resid.cpu().numpy(), ax=ax)\n",
    "# ax.set(yscale=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_out_0 = cache[\"attn_out\", 0].flatten(0, 1)\n",
    "V_base = model.W_E_pos\n",
    "\n",
    "U_out_0, S_out_0, Vh_out_0 = torch.linalg.svd(V_out_0, full_matrices=False)\n",
    "U_base, S_base, Vh_base = torch.linalg.svd(V_base, full_matrices=False)\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(10, 4))\n",
    "sns.lineplot(x=range(len(S_out_0)), y=S_out_0.cpu(), ax=axs[0])\n",
    "sns.lineplot(x=range(len(S_base)), y=S_base.cpu(), ax=axs[1])\n",
    "axs[0].axvline(85, color=\"black\", linestyle=\"--\", alpha=0.5)\n",
    "axs[0].set(yscale=\"log\")\n",
    "axs[1].set(yscale=\"log\")\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "# V_base = V_base / torch.linalg.norm(V_base, dim=-1, keepdim=True)\n",
    "# V_out_0 = V_out_0 / torch.linalg.norm(V_out_0, dim=-1, keepdim=True)\n",
    "\n",
    "# sim_out_0_out_0 = V_out_0 @ V_out_0.transpose(-2, -1)\n",
    "sim_out_0_base = cosine_sim(V_out_0, V_base)\n",
    "sim_base_base = cosine_sim(V_base, V_base)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# sns.histplot(sim_out_0_out_0.flatten().cpu(), ax=ax, label=\"out_0 vs out_0\")\n",
    "# sns.histplot(sim_out_0_base.flatten().cpu(), ax=ax, label=\"out_0 vs base\")\n",
    "sns.histplot(sim_base_base.flatten().cpu(), ax=ax, label=\"base vs base\", stat=\"density\")\n",
    "# ax.set(yscale=\"log\")\n",
    "# ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_1 = torch.tensor([[ds_train.bos_token, 5, 1, 4, 2, 8, 0]])\n",
    "game_2 = torch.tensor([[ds_train.bos_token, 5, 4, 1, 2, 8, 0]])\n",
    "print(tensor_to_state(game_1[0]))\n",
    "print(tensor_to_state(game_2[0]))\n",
    "\n",
    "clean_logits, clean_cache = model.run_with_cache(game_1)\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(game_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_head_vector(corrupted_head_vector, hook, seq_idx, head_index):\n",
    "    corrupted_head_vector[:, seq_idx, head_index, :] = corrupted_cache[hook.name][\n",
    "        :, seq_idx, head_index, :\n",
    "    ]\n",
    "    return corrupted_head_vector\n",
    "\n",
    "\n",
    "def logit_diff(patched_logits):\n",
    "    # terminate_idx = [9, 10, 11]\n",
    "    # other_idx = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "    terminate_idx = list(range(9))\n",
    "    other_idx = [10]\n",
    "    seq_idx = -1\n",
    "    diff_patched = patched_logits[:, seq_idx, terminate_idx].mean(-1) - patched_logits[\n",
    "        :, seq_idx, other_idx\n",
    "    ].mean(-1)\n",
    "    diff_clean = clean_logits[:, seq_idx, terminate_idx].mean(-1) - clean_logits[\n",
    "        :, seq_idx, other_idx\n",
    "    ].mean(-1)\n",
    "    rel_change = diff_patched - diff_clean\n",
    "    return rel_change.mean()\n",
    "\n",
    "\n",
    "patched_residual_stream_diff = torch.zeros(\n",
    "    model.cfg.n_layers, model.cfg.n_heads, game_1.shape[1]\n",
    ")\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    for head_index in range(model.cfg.n_heads):\n",
    "        for seq_idx in range(game_1.shape[1]):\n",
    "            hook_fn = partial(patch_head_vector, head_index=head_index, seq_idx=seq_idx)\n",
    "            patched_logits = model.run_with_hooks(\n",
    "                game_1,\n",
    "                fwd_hooks=[(utils.get_act_name(\"z\", layer, \"attn\"), hook_fn)],\n",
    "                return_type=\"logits\",\n",
    "            )\n",
    "            patched_logit_diff = logit_diff(patched_logits)\n",
    "            patched_residual_stream_diff[\n",
    "                layer, head_index, seq_idx\n",
    "            ] = patched_logit_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=model.cfg.n_heads, figsize=(8, 8))\n",
    "_min = patched_residual_stream_diff.min().item()\n",
    "_max = patched_residual_stream_diff.max().item()\n",
    "for i, ax in enumerate(axs):\n",
    "    sns.heatmap(\n",
    "        patched_residual_stream_diff[:, i].cpu(),\n",
    "        ax=ax,\n",
    "        cmap=\"RdBu\",\n",
    "        square=True,\n",
    "        center=0,\n",
    "        vmin=_min,\n",
    "        vmax=_max,\n",
    "    )\n",
    "    ax.set(\n",
    "        xlabel=\"Seq\",\n",
    "        ylabel=\"Layer\",\n",
    "        xticklabels=ds_train.decode(game_1[0]),\n",
    "    )\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tic-tac-gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
