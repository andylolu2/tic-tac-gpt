{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from lightning import fabric\n",
    "from transformer_lens import (\n",
    "    HookedTransformer,\n",
    "    HookedTransformerConfig,\n",
    "    FactoredMatrix,\n",
    "    ActivationCache,\n",
    ")\n",
    "from transformer_lens import utils\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier, MultiOutputRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import log_loss\n",
    "from einops import einsum, rearrange, unpack\n",
    "from matplotlib import cm, colors\n",
    "\n",
    "from tic_tac_gpt.data import TicTacToeDataset, TicTacToeState, tensor_to_state\n",
    "\n",
    "import tic_tac_gpt.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "torch.set_default_device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = Path(\"out/model/exp20\")\n",
    "\n",
    "with open(checkpoint_dir / \"config.pkl\", \"rb\") as f:\n",
    "    config: HookedTransformerConfig = pickle.load(f)\n",
    "model = HookedTransformer(config)\n",
    "F = fabric.Fabric(precision=\"16-mixed\")\n",
    "state_dict = F.load(checkpoint_dir / \"model_40000.pt\")\n",
    "model.load_and_process_state_dict(state_dict)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = TicTacToeDataset.from_file(Path(\"out/dataset/50_50/train.jsonl\"))\n",
    "ds_test = TicTacToeDataset.from_file(Path(\"out/dataset/50_50/test.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x,) = random.choice(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_logit(logit, ax1=None, ax2=None):\n",
    "    probs = logit\n",
    "    board = [[0] * 3 for _ in range(3)]\n",
    "    special = {}\n",
    "    for i, p in enumerate(probs.tolist()):\n",
    "        move = TicTacToeDataset.decode_one(i)\n",
    "        if isinstance(move, int):\n",
    "            board[move // 3][move % 3] = p\n",
    "        else:\n",
    "            special[move] = p\n",
    "\n",
    "    if ax1 is None or ax2 is None:\n",
    "        fig, axs = plt.subplots(\n",
    "            ncols=2, figsize=(6, 2), gridspec_kw={\"width_ratios\": [1.5, 1]}\n",
    "        )\n",
    "    else:\n",
    "        axs = (ax1, ax2)\n",
    "    sns.heatmap(\n",
    "        board, annot=True, fmt=\".2f\", ax=axs[0], cmap=\"RdBu\", center=0, square=True\n",
    "    )\n",
    "    sns.barplot(x=list(special.keys()), y=list(special.values()), ax=axs[1])\n",
    "    axs[1].set(title=\"Special moves\")\n",
    "\n",
    "\n",
    "def visualise_board(state):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(state.board, cmap=\"gray\", aspect=\"equal\", vmin=0, vmax=2)\n",
    "    ax.set(xticks=[], yticks=[], title=\"Board\")\n",
    "    return fig\n",
    "\n",
    "\n",
    "print(tensor_to_state(x))\n",
    "visualise_logit(model(x)[0, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_games = ds_train[torch.randperm(len(ds_train))[:2048]][0]\n",
    "focus_games_test = ds_test[torch.randperm(len(ds_test))[:2048]][0]\n",
    "focus_games.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, cache = model.run_with_cache(focus_games[:, :-1])\n",
    "cache.compute_head_results()\n",
    "logits_test, cache_test = model.run_with_cache(focus_games_test[:, :-1])\n",
    "cache_test.compute_head_results()\n",
    "print(cache.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kurtosis\n",
    "\n",
    "a = cache[\"resid_mid\", 0]\n",
    "\n",
    "sns.histplot(a.flatten().cpu().numpy(), bins=100)\n",
    "# kurtosis(a.flatten().cpu().numpy(), fisher=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_states(game: torch.Tensor):\n",
    "    for i in range(1, game.shape[0] + 1):\n",
    "        state = tensor_to_state(game[:i])\n",
    "        yield state\n",
    "        if state.result != \"in_progress\":\n",
    "            break\n",
    "\n",
    "\n",
    "def board_to_tensor(board):\n",
    "    return torch.tensor(board).flatten(-2, -1)\n",
    "\n",
    "\n",
    "board_states = extract_states(focus_games[7])\n",
    "for s in board_states:\n",
    "    print(s)\n",
    "    print(board_to_tensor(s.board))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_layer = 0\n",
    "\n",
    "\n",
    "def invert(x):\n",
    "    y = x.clone()\n",
    "    y[x == 1] = 2\n",
    "    y[x == 2] = 1\n",
    "    return y\n",
    "\n",
    "\n",
    "def remap(x):\n",
    "    y = x.clone()\n",
    "    y[x == 0] = 0\n",
    "    y[x == 1] = 1\n",
    "    y[x == 2] = 2\n",
    "    return y\n",
    "\n",
    "\n",
    "def extract_XY(games, cache):\n",
    "    activations = cache[\"resid_mid\", probe_layer]\n",
    "    # activations = cache[\"attn_out\", probe_layer]\n",
    "    # activations = cache[\"z\", probe_layer, \"attn\"][:, :, 2, :]\n",
    "    X, Y = [], []\n",
    "    for game, game_activation in zip(games, activations):\n",
    "        for i, (state, activation) in enumerate(\n",
    "            zip(extract_states(game), game_activation)\n",
    "        ):\n",
    "            X.append(activation)\n",
    "            target = board_to_tensor(state.board)\n",
    "            if i % 2 == 0:\n",
    "                target = invert(target)\n",
    "            # target = remap(target)\n",
    "            Y.append(target)\n",
    "            # Y.append(torch.nn.functional.one_hot(target, 3))\n",
    "    X, Y = torch.stack(X), torch.stack(Y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "X, Y = extract_XY(focus_games, cache)\n",
    "X_test, Y_test = extract_XY(focus_games_test, cache_test)\n",
    "X.shape, Y.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np = X.cpu().numpy()\n",
    "Y_np = Y.cpu().numpy()\n",
    "# Y_np = Y.flatten(-2, -1).cpu().numpy()\n",
    "X_test_np = X_test.cpu().numpy()\n",
    "Y_test_np = Y_test.cpu().numpy()\n",
    "# Y_test_np = Y_test.flatten(-2, -1).cpu().numpy()\n",
    "\n",
    "regressor = MultiOutputRegressor(LinearRegression())\n",
    "regressor.fit(X_np, Y_np)\n",
    "print(regressor.score(X_np, Y_np), regressor.score(X_test_np, Y_test_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np = X.cpu().numpy()\n",
    "Y_np = Y.cpu().numpy()\n",
    "X_test_np = X_test.cpu().numpy()\n",
    "Y_test_np = Y_test.cpu().numpy()\n",
    "\n",
    "classifier = MultiOutputClassifier(LogisticRegression(max_iter=1000))\n",
    "classifier.fit(X_np, Y_np)\n",
    "\n",
    "Y_pred = np.array(classifier.predict_proba(X_np)).transpose(1, 0, 2)\n",
    "Y_test_pred = np.array(classifier.predict_proba(X_test_np)).transpose(1, 0, 2)\n",
    "print(classifier.score(X_np, Y_np), classifier.score(X_test_np, Y_test_np))\n",
    "print(\n",
    "    log_loss(Y_np.reshape(-1), Y_pred.reshape(-1, Y_pred.shape[-1])),\n",
    "    log_loss(Y_test_np.reshape(-1), Y_test_pred.reshape(-1, Y_test_pred.shape[-1])),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attn(x, pattern):\n",
    "    \"\"\"Input shape (nh s s)\"\"\"\n",
    "    fig, axs = plt.subplots(\n",
    "        ncols=pattern.shape[0] + 1,\n",
    "        figsize=(pattern.shape[0] * 4, 4),\n",
    "        gridspec_kw={\"width_ratios\": [1] * pattern.shape[0] + [0.07]},\n",
    "    )\n",
    "    for i in range(pattern.shape[0]):\n",
    "        ax = axs[i]\n",
    "        p = pattern[i]\n",
    "        sns.heatmap(\n",
    "            p,\n",
    "            ax=ax,\n",
    "            cmap=\"viridis\",\n",
    "            square=True,\n",
    "            xticklabels=[f\"\\\\texttt{{{i}}}\" for i in x],\n",
    "            yticklabels=[f\"\\\\texttt{{{i}}}\" for i in x],\n",
    "            cbar=False,\n",
    "            linewidth=0.5,\n",
    "            linecolor=\"black\",\n",
    "            clip_on=False,\n",
    "        )\n",
    "        ax.set(title=f\"Head {i}\")\n",
    "\n",
    "    im = cm.ScalarMappable(cmap=\"viridis\", norm=colors.Normalize(vmin=0, vmax=1))\n",
    "    fig.colorbar(im, cax=axs[-1])\n",
    "    return fig\n",
    "\n",
    "\n",
    "idx = random.randint(0, len(focus_games))\n",
    "game = focus_games[idx]\n",
    "# game = torch.tensor([ds_train.bos_token, 1, 5, 4, 8, 7])\n",
    "print(tensor_to_state(game))\n",
    "\n",
    "_, tmp_cache = model.run_with_cache(game)\n",
    "fig = plot_attn(\n",
    "    TicTacToeDataset.decode(game),\n",
    "    tmp_cache[\"pattern\", 0][0].cpu().numpy(),\n",
    ")\n",
    "# fig.savefig(\"out/figs/attn_pattern.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(x: torch.Tensor, y: torch.Tensor):\n",
    "    x = torch.nn.functional.normalize(x, dim=-1)\n",
    "    y = torch.nn.functional.normalize(y, dim=-1)\n",
    "    return torch.tensordot(x, y, dims=([-1], [-1]))\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    x = x - x.mean(-1, keepdim=True)\n",
    "    scale = (x.pow(2).mean(-1, keepdim=True)).sqrt()\n",
    "    return x / scale\n",
    "\n",
    "\n",
    "x = torch.randn(4, 5)\n",
    "y = torch.randn(3, 5)\n",
    "assert (cosine_sim(x, y) == cosine_sim(y, x).T).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_idx = 1\n",
    "\n",
    "seq_activations = [[] for _ in range(ds_train.max_seq_len - 1)]\n",
    "for activations, game in zip(cache[\"resid_mid\", 0], focus_games_test):\n",
    "    # activations = activations[:, head_idx]\n",
    "    for i, (activation, move) in enumerate(zip(activations, game)):\n",
    "        move = move.item()\n",
    "        if move in (0, 1, 2, 3, 4, 5, 6, 7, 8, ds_train.bos_token):\n",
    "            seq_activations[i].append(activation)\n",
    "seq_activations = [torch.stack(s) for s in seq_activations]\n",
    "print([s.shape for s in seq_activations])\n",
    "\n",
    "data = []\n",
    "for i, activations in enumerate(seq_activations):\n",
    "    singulars = torch.linalg.svdvals(activations).cpu().numpy()\n",
    "    singulars = singulars / singulars[0]\n",
    "    for j, s in enumerate(sorted(singulars, reverse=True)):\n",
    "        data.append({\"idx\": j, \"singular\": s, \"position\": str(i)})\n",
    "\n",
    "ax = sns.lineplot(data=pd.DataFrame(data), x=\"idx\", y=\"singular\", hue=\"position\")\n",
    "# for x in (1, 9, 10, 11, 12, 13, 15, 17, 19, 20):\n",
    "for x in (0, 8, 10, 16, 20, 22, 24, 26, 29, 32, 35, 38):\n",
    "    ax.axvline(x, color=\"black\", linestyle=\"--\", alpha=0.5)\n",
    "ax.set(yscale=\"log\", ylim=(1e-3, 1), xlim=(-1, 50))\n",
    "# ax.set(ylim=(None, None), xlim=(-1, 40))\n",
    "# ax.set(yscale=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_1 = torch.tensor([[ds_train.bos_token, 0, 5, 1, 8, 2]])\n",
    "game_2 = torch.tensor([[ds_train.bos_token, 0, 5, 1, 8, 2]])\n",
    "print(tensor_to_state(game_1[0]))\n",
    "print(tensor_to_state(game_2[0]))\n",
    "\n",
    "clean_logits, clean_cache = model.run_with_cache(game_1)\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(game_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_head_vector(corrupted_head_vector, hook, head_index):\n",
    "    corrupted_head_vector[:, -2, head_index, :] = corrupted_cache[hook.name][\n",
    "        :, -1, head_index, :\n",
    "    ]\n",
    "    return corrupted_head_vector\n",
    "\n",
    "\n",
    "def logit_diff(patched_logits):\n",
    "    # terminate_idx = [9, 10, 11]\n",
    "    # other_idx = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "    terminate_idx = [10]\n",
    "    other_idx = [9]\n",
    "    seq_idx = -2\n",
    "    diff_patched = patched_logits[:, seq_idx, terminate_idx].mean(-1) - patched_logits[\n",
    "        :, seq_idx, other_idx\n",
    "    ].mean(-1)\n",
    "    diff_clean = clean_logits[:, seq_idx, terminate_idx].mean(-1) - clean_logits[\n",
    "        :, seq_idx, other_idx\n",
    "    ].mean(-1)\n",
    "    rel_change = diff_patched  # - diff_clean\n",
    "    return rel_change.mean()\n",
    "\n",
    "\n",
    "patched_residual_stream_diff = torch.zeros(\n",
    "    model.cfg.n_layers, model.cfg.n_heads, game_1.shape[1]\n",
    ")\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    for head_index in range(model.cfg.n_heads):\n",
    "        for seq_idx in range(game_1.shape[1]):\n",
    "            hook_fn = partial(patch_head_vector, head_index=head_index)\n",
    "            patched_logits = model.run_with_hooks(\n",
    "                game_1,\n",
    "                fwd_hooks=[(utils.get_act_name(\"v\", layer, \"attn\"), hook_fn)],\n",
    "                return_type=\"logits\",\n",
    "            )\n",
    "            patched_logit_diff = logit_diff(patched_logits)\n",
    "            patched_residual_stream_diff[\n",
    "                layer, head_index, seq_idx\n",
    "            ] = patched_logit_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=model.cfg.n_heads, figsize=(8, 8))\n",
    "_min = patched_residual_stream_diff.min().item()\n",
    "_max = patched_residual_stream_diff.max().item()\n",
    "for i, ax in enumerate(axs):\n",
    "    sns.heatmap(\n",
    "        patched_residual_stream_diff[:, i].cpu(),\n",
    "        ax=ax,\n",
    "        cmap=\"RdBu\",\n",
    "        square=True,\n",
    "        center=0,\n",
    "        vmin=_min,\n",
    "        vmax=_max,\n",
    "    )\n",
    "    ax.set(\n",
    "        xlabel=\"Seq\",\n",
    "        ylabel=\"Layer\",\n",
    "        xticklabels=ds_train.decode(game_1[0]),\n",
    "    )\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_idx = random.randint(0, 2048)\n",
    "game_tensor = focus_games[game_idx]\n",
    "# game_tensor = torch.tensor([ds_train.bos_token, 8, 6, 0, 4, 1, 5, 2])\n",
    "# game_tensor = torch.tensor([ds_train.bos_token, 3, 6, 1, 5, 4, 8, 7])\n",
    "game = tensor_to_state(game_tensor)\n",
    "\n",
    "print(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_idx = 1\n",
    "s = len(game) + 1\n",
    "\n",
    "\n",
    "def logit_contribution(activation):\n",
    "    return torch.matmul(activation, model.W_U[:, out_idx])\n",
    "\n",
    "\n",
    "head_results = cache.stack_head_results()\n",
    "head_results = cache.apply_ln_to_stack(head_results)\n",
    "head_results = rearrange(\n",
    "    head_results, \"(l nh) b s d -> b s l nh d\", l=model.cfg.n_layers\n",
    ")\n",
    "\n",
    "out = torch.zeros(model.cfg.n_layers, model.cfg.n_heads, s)\n",
    "for layer_idx in range(model.cfg.n_layers):\n",
    "    for head_idx in range(model.cfg.n_heads):\n",
    "        for seq_idx in range(s):\n",
    "            out[layer_idx, head_idx, seq_idx] = logit_contribution(\n",
    "                head_results[game_idx, seq_idx, layer_idx, head_idx]\n",
    "            )\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    ncols=3,\n",
    "    nrows=s,\n",
    "    figsize=(10, 18),\n",
    "    gridspec_kw={\"width_ratios\": [3, 1, 3]},\n",
    ")\n",
    "# v_min = out.min().item()\n",
    "# v_max = out.max().item()\n",
    "for seq_idx in range(s):\n",
    "    visualise_logit(logits[game_idx, seq_idx], axs[seq_idx, 0], axs[seq_idx, 1])\n",
    "    sns.heatmap(\n",
    "        out[:, :, seq_idx].cpu(),\n",
    "        ax=axs[seq_idx, 2],\n",
    "        cmap=\"RdBu\",\n",
    "        center=0,\n",
    "        square=True,\n",
    "    )\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_idx = len(game) - 2\n",
    "seq_idx = len(game)\n",
    "\n",
    "\n",
    "def logit_contribution(activation, out_idx):\n",
    "    return torch.matmul(activation, model.W_U[:, out_idx])\n",
    "\n",
    "\n",
    "tmp_logits, tmp_cache = model.run_with_cache(game_tensor)\n",
    "all_acts = tmp_cache.get_full_resid_decomposition(pos_slice=seq_idx, apply_ln=True)\n",
    "# all_acts = tmp_cache.apply_ln_to_stack(all_acts, pos_slice=seq_idx)\n",
    "all_acts = all_acts[:, 0, :]\n",
    "\n",
    "heads, neurons, emb, pos, bias = unpack(\n",
    "    all_acts,\n",
    "    [\n",
    "        [model.cfg.n_layers, model.cfg.n_heads],\n",
    "        [model.cfg.n_layers, model.cfg.d_mlp],\n",
    "        [1],\n",
    "        [1],\n",
    "        [1],\n",
    "    ],\n",
    "    \"* d\",\n",
    ")\n",
    "print(game)\n",
    "for name, act in {\"emb\": emb, \"pos\": pos, \"bias\": bias}.items():\n",
    "    logits_contributions = [\n",
    "        logit_contribution(act, i).item() for i in range(model.cfg.d_vocab_out)\n",
    "    ]\n",
    "    print(f\"{name:>4}: {' '.join(f'{x:>5.2f}' for x in logits_contributions)}\")\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    ncols=2,\n",
    "    nrows=model.cfg.d_vocab_out,\n",
    "    figsize=(12, 1 * model.cfg.d_vocab_out),\n",
    "    gridspec_kw={\"width_ratios\": [1, 10]},\n",
    "    sharey=\"row\",\n",
    "    sharex=\"col\",\n",
    ")\n",
    "for i in range(model.cfg.d_vocab_out):\n",
    "    heads_logits = logit_contribution(heads, i).flatten()\n",
    "    sns.scatterplot(\n",
    "        x=np.arange(heads_logits.numel()), y=heads_logits.cpu().numpy(), ax=axs[i, 0]\n",
    "    )\n",
    "    neurons_logits = logit_contribution(neurons, i).flatten()\n",
    "    sns.scatterplot(\n",
    "        x=np.arange(neurons_logits.numel()),\n",
    "        y=neurons_logits.cpu().numpy(),\n",
    "        ax=axs[i, 1],\n",
    "    )\n",
    "    # label outliers\n",
    "    for x, y in enumerate(neurons_logits):\n",
    "        if y.abs() > 1:\n",
    "            axs[i, 1].text(x, y, f\"{x}\", fontsize=8, ha=\"center\", va=\"center\")\n",
    "    axs[i, 0].set(ylabel=f\"{ds_train.decode_one(i)}\")\n",
    "fig.tight_layout()\n",
    "visualise_logit(tmp_logits[0, seq_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_in = model.W_in[0, :, :]\n",
    "\n",
    "E_base = torch.concat([model.W_E[:9], model.W_pos[:10]], dim=0)\n",
    "E = E_base @ model.OV[0, :].AB\n",
    "E = torch.concat([E, E_base.unsqueeze(0)], dim=0)\n",
    "E = torch.concat([E, E.sum(0, keepdim=True)], dim=0)\n",
    "E = normalize(E)\n",
    "\n",
    "# sim = cosine_sim(V_in.T, E)\n",
    "sim = torch.tensordot(V_in.T, E, dims=([-1], [-1]))\n",
    "\n",
    "print(f\"{sim.shape=}\")\n",
    "fig, axs = plt.subplots(ncols=len(E), figsize=(20, 100))\n",
    "order_ = torch.argsort(torch.amax(sim, dim=(-2, -1)), descending=True)[:400]\n",
    "for i, ax in enumerate(axs):\n",
    "    sns.heatmap(\n",
    "        sim[order_, i].cpu(),\n",
    "        cmap=\"RdBu\",\n",
    "        center=0,\n",
    "        square=True,\n",
    "        ax=ax,\n",
    "        # vmin=0,\n",
    "        yticklabels=order_.cpu().numpy(),\n",
    "        cbar=True,\n",
    "    )\n",
    "    ax.set(title=f\"Head {i}\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = torch.concat([model.W_E[:9], model.W_pos[:10]], dim=0)\n",
    "print(f\"{E.shape=}\")\n",
    "# P = model.W_pos[:10]\n",
    "\n",
    "V1 = E @ model.OV[0, 0].AB\n",
    "V2 = E @ model.OV[0, 1].AB\n",
    "# V2 = E\n",
    "print(f\"{V1.shape=} {V2.shape=}\")\n",
    "\n",
    "# sim = cosine_sim(V1, V2)\n",
    "sim = torch.tensordot(V1, V2, dims=([-1], [-1]))\n",
    "# assert (sim == cosine_sim(V2, V1).T).all()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "labels = [ds_train.decode_one(i) for i in range(9)] + list(range(10))\n",
    "sns.heatmap(\n",
    "    sim.cpu(),\n",
    "    cmap=\"RdBu\",\n",
    "    center=0,\n",
    "    square=True,\n",
    "    ax=ax,\n",
    "    yticklabels=labels,\n",
    "    xticklabels=labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocab = [12, 0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "E = model.W_E[target_vocab]\n",
    "P = model.W_pos[:10]\n",
    "\n",
    "\n",
    "def labels_for(A):\n",
    "    if A is E:\n",
    "        labels = [ds_train.decode_one(i) for i in target_vocab]\n",
    "        return [l if isinstance(l, str) else f\"$E_{l}$\" for l in labels]\n",
    "    elif A is P:\n",
    "        return [f\"$P_{i}$\" for i in range(len(A))]\n",
    "    assert False\n",
    "\n",
    "\n",
    "def title_for(A):\n",
    "    if A is E:\n",
    "        return \"W_E\"\n",
    "    elif A is P:\n",
    "        return \"W_P\"\n",
    "    raise ValueError(\"Unknown matrix\")\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(9, 5))\n",
    "subfigs = fig.subfigures(1, 3, width_ratios=[1, 1, 0.07])\n",
    "\n",
    "for i, subfig in enumerate(subfigs[:2]):\n",
    "    subfig.suptitle(f\"Head {i}\", fontsize=\"xx-large\")\n",
    "    subfig.supxlabel(\"Key\", fontsize=\"x-large\")\n",
    "    subfig.supylabel(\"Query\", fontsize=\"x-large\")\n",
    "    axs = subfig.subplots(2, 2)\n",
    "    for j, (A, B) in enumerate([(E, E), (E, P), (P, E), (P, P)]):\n",
    "        sim = A @ model.QK[0, i].AB @ B.T\n",
    "        if A is P and B is P:\n",
    "            sim = torch.tril(sim)\n",
    "        if A is E:\n",
    "            sim[0, 1:] = 0\n",
    "        sns.heatmap(\n",
    "            sim.cpu(),\n",
    "            ax=axs.flat[j],\n",
    "            vmax=0.06,\n",
    "            center=0,\n",
    "            yticklabels=labels_for(A),\n",
    "            xticklabels=labels_for(B),\n",
    "            cmap=\"RdBu\",\n",
    "            square=True,\n",
    "            cbar=False,\n",
    "            linewidth=0.5,\n",
    "            linecolor=\"black\",\n",
    "            clip_on=False,\n",
    "        )\n",
    "        # axs.flat[j].set(title=f\"${title_for(A)}^T W_Q^T W_K {title_for(B)}$\")\n",
    "\n",
    "cbar_ax = subfigs[-1].add_subplot()\n",
    "im = cm.ScalarMappable(cmap=\"RdBu\", norm=colors.Normalize(vmin=-0.06, vmax=0.06))\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "# fig.tight_layout()\n",
    "\n",
    "fig.savefig(\"out/figs/attention.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linalg.norm(model.W_E_pos, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = torch.concat([model.W_E[:9], model.W_pos[:10]], dim=0)\n",
    "print(f\"{E.shape=}\")\n",
    "# P = model.W_pos[:10]\n",
    "\n",
    "# sim = cosine_sim(E, E)\n",
    "sim = torch.tensordot(E, E, dims=([-1], [-1]))\n",
    "sim.fill_diagonal_(0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "labels = [ds_train.decode_one(i) for i in range(9)] + list(range(10))\n",
    "sns.heatmap(\n",
    "    sim.cpu(),\n",
    "    cmap=\"RdBu\",\n",
    "    center=0,\n",
    "    square=True,\n",
    "    ax=ax,\n",
    "    yticklabels=labels,\n",
    "    xticklabels=labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = torch.tensordot(model.W_out[0], model.W_U.T, dims=([-1], [-1]))\n",
    "# sim = cosine_sim(model.W_out[0], model.W_U.T)\n",
    "sim = sim - torch.amin(sim, dim=-1, keepdim=True)\n",
    "order_ = torch.argsort(torch.amax(sim.abs(), dim=(-1)), descending=True)\n",
    "sim = sim[order_]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 100))\n",
    "sns.heatmap(\n",
    "    sim.cpu(),\n",
    "    cmap=\"RdBu\",\n",
    "    center=0,\n",
    "    square=True,\n",
    "    ax=ax,\n",
    "    yticklabels=order_.tolist(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qk_solbol_indices(head_idx):\n",
    "    EP = rearrange(model.W_E, \"v d -> 1 v d\") + rearrange(model.W_pos, \"p d -> p 1 d\")\n",
    "    A = torch.tensordot(EP @ model.QK[0, head_idx].AB, EP, dims=([-1], [-1]))\n",
    "    g0 = torch.mean(A)\n",
    "\n",
    "    def g1_fn(i):\n",
    "        dims = [j for j in range(A.ndim) if j != i]\n",
    "        return torch.mean(A, dim=dims) - g0\n",
    "\n",
    "    g1 = [g1_fn(i) for i in range(A.ndim)]\n",
    "\n",
    "    def g2_fn(i, j):\n",
    "        assert i < j\n",
    "        dims = [k for k in range(A.ndim) if k != i and k != j]\n",
    "        return torch.mean(A, dim=dims) - g0 - g1[i].unsqueeze(-1) - g1[j]\n",
    "\n",
    "    g2 = [[g2_fn(i, j) for i in range(j)] for j in range(A.ndim)]\n",
    "\n",
    "    v = torch.var(A, unbiased=False).item()\n",
    "    v1 = [torch.var(g, unbiased=False).item() for g in g1]\n",
    "    v2 = [[torch.var(g, unbiased=False).item() for g in gs] for gs in g2]\n",
    "\n",
    "    sobol = {\n",
    "        \"$P_Q$\": v1[0] / v,\n",
    "        \"$E_Q$\": v1[1] / v,\n",
    "        \"$P_K$\": v1[2] / v,\n",
    "        \"$P_E$\": v1[3] / v,\n",
    "        \"$E_Q, P_Q$\": v2[1][0] / v,\n",
    "        \"$P_K, P_Q$\": v2[2][0] / v,\n",
    "        \"$P_K, E_Q$\": v2[2][1] / v,\n",
    "        \"$E_K, P_Q$\": v2[3][0] / v,\n",
    "        \"$E_K, E_Q$\": v2[3][1] / v,\n",
    "        \"$E_K, P_K$\": v2[3][2] / v,\n",
    "    }\n",
    "    sobol[\"Higher\\norder\"] = 1 - sum(sobol.values())\n",
    "    return sobol\n",
    "\n",
    "\n",
    "data = []\n",
    "for head_idx in range(model.cfg.n_heads):\n",
    "    data.append({\"Head\": head_idx, **qk_solbol_indices(head_idx)})\n",
    "df = pd.DataFrame(data).melt(id_vars=\"Head\")\n",
    "\n",
    "# sort by head 0 value\n",
    "order = df[df[\"Head\"] == 0].sort_values(\"value\", ascending=False)[\"variable\"].tolist()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "sns.barplot(data=df, x=\"variable\", y=\"value\", hue=\"Head\", ax=ax, order=order)\n",
    "ax.set(\n",
    "    ylabel=\"Explained variance of attention\",\n",
    "    xlabel=\"Interaction\",\n",
    ")\n",
    "fig.savefig(\"out/figs/sobol.pdf\", bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tic-tac-gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
